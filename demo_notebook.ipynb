{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook showcases the key features and processing steps of the **tiny_corpus_prep** library, a Polars-based corpus preparation tool for training tiny GPT-like models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation <a name=\"setup\"></a>\n",
    "\n",
    "First, let's import the necessary modules and verify the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import polars as pl\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import tiny_corpus_prep components (main API)\n",
    "from tiny_corpus_prep import (\n",
    "    process_corpus,\n",
    "    DataPipeline,\n",
    "    read_parquet,\n",
    "    write_parquet_with_stats,\n",
    "    filter_by_readability,\n",
    "    filter_by_keywords,\n",
    "    is_middle_school_level,\n",
    "    CustomFunctionAnnotator,\n",
    "    generate_stats,\n",
    ")\n",
    "\n",
    "# Import internal utilities (not part of public API but useful for demos)\n",
    "from tiny_corpus_prep.normalize import normalize_text\n",
    "from tiny_corpus_prep.synonyms import SynonymMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data <a name=\"download\"></a>\n",
    "\n",
    "Before processing, you can download real datasets from Wikipedia or FineWeb-edu. \n",
    "\n",
    "### Data Sources:\n",
    "- **Wikipedia**: Simple English Wikipedia - great for general knowledge\n",
    "- **FineWeb-edu**: Pre-filtered educational web content from HuggingFace\n",
    "\n",
    "**Note**: These downloads can be large (100MB - 2GB+) and may take several minutes. The cells below are commented out by default - uncomment to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data directory: demo_data_downloads\n",
      "Full path: /home/gillus/tiny_corpus_prep/demo_data_downloads\n"
     ]
    }
   ],
   "source": [
    "# Create a data directory for downloads\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"demo_data_downloads\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created data directory: {data_dir}\")\n",
    "print(f\"Full path: {data_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Download Wikipedia Data\n",
    "\n",
    "Downloads Simple English Wikipedia, extracts articles, and converts to parquet format.\n",
    "\n",
    "**Requirements**: `bzip2` (system), `wikiextractor` (Python package)\n",
    "\n",
    "**Size**: ~500MB compressed, expands to ~2GB\n",
    "\n",
    "Wikipedia content is released under the Creative Commons Attribution-ShareAlike (CC BY-SA 4.0) license. This means that any model trained on Wikipedia data, as well as any outputs that constitute derivative works, must comply with CC BY-SA requirements. In practice, you must ensure that attribution is preserved (e.g., acknowledging Wikipedia as a source in documentation), and any redistributed derivative content is shared under the same license. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "tiny_corpus_prep - Data Download Tool (Step 0)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Source: wikipedia\n",
      "  Output directory: demo_data_downloads/wikipedia\n",
      "  Date: 20251020\n",
      "\n",
      "\n",
      "=== Downloading Simple Wikipedia (20251020) ===\n",
      "✓ XML file already exists: demo_data_downloads/wikipedia/simplewiki-20251020-pages-articles-multistream.xml\n",
      "\n",
      "=== Extracting Wikipedia content ===\n",
      "Running WikiExtractor (output to demo_data_downloads/wikipedia/extracted)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Preprocessing 'demo_data_downloads/wikipedia/simplewiki-20251020-pages-articles-multistream.xml' to collect template definitions: this may take some time.\n",
      "INFO: Preprocessed 100000 pages\n",
      "INFO: Preprocessed 200000 pages\n",
      "INFO: Preprocessed 300000 pages\n",
      "INFO: Preprocessed 400000 pages\n",
      "INFO: Preprocessed 500000 pages\n",
      "INFO: Loaded 43176 templates in 23.2s\n",
      "INFO: Starting page extraction from demo_data_downloads/wikipedia/simplewiki-20251020-pages-articles-multistream.xml.\n",
      "INFO: Using 7 extract processes.\n",
      "INFO: Extracted 100000 articles (3791.7 art/s)\n",
      "INFO: Extracted 200000 articles (3879.7 art/s)\n",
      "INFO: Extracted 300000 articles (4301.3 art/s)\n",
      "INFO: Finished 7-process extraction of 382659 articles in 94.1s (4067.0 art/s)\n",
      "Processing files:   2%|▏         | 6/246 [00:00<00:04, 54.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Converting to Parquet ===\n",
      "Found 246 JSON files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 246/246 [00:04<00:00, 56.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created DataFrame with 382658 rows\n",
      "shape: (5, 7)\n",
      "┌───────────────┬──────────────┬──────────────┬──────────────┬──────────────┬───────┬──────────────┐\n",
      "│ filename      ┆ title        ┆ text         ┆ number_of_ch ┆ number_of_wo ┆ topic ┆ text_quality │\n",
      "│ ---           ┆ ---          ┆ ---          ┆ aracters     ┆ rds          ┆ ---   ┆ ---          │\n",
      "│ str           ┆ str          ┆ str          ┆ ---          ┆ ---          ┆ str   ┆ i64          │\n",
      "│               ┆              ┆              ┆ i64          ┆ i64          ┆       ┆              │\n",
      "╞═══════════════╪══════════════╪══════════════╪══════════════╪══════════���═══╪═══════╪══════════════╡\n",
      "│ demo_data_dow ┆ Beverly      ┆ Beverly      ┆ 262          ┆ 41           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Hills Madam  ┆ Hills Madam  ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ (also know…  ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ The Light at ┆ The Light at ┆ 269          ┆ 41           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ the Edge of  ┆ the Edge of  ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆ the W…       ┆ the W…       ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ INTLAB       ┆ INTLAB       ┆ 607          ┆ 90           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆              ┆ (INTerval    ┆              ┆              ���       ┆              │\n",
      "│ dia/…         ┆              ┆ LABoratory)  ┆              ┆              ┆       ┆              │\n",
      "│               ┆              ┆ i…           ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Lemon        ┆ Lemon        ┆ 263          ┆ 44           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Popsicle     ┆ Popsicle     ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ (Israeli:    ┆              ┆              ┆       ┆              │\n",
      "│               ┆              ┆ Eskim…       ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Ace Eli and  ┆ Ace Eli and  ┆ 230          ┆ 37           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Rodger of    ┆ Rodger of    ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆ the Skie…    ┆ the Skie…    ┆              ┆              ┆       ┆              │\n",
      "└───────────────┴──────────────┴──────────────┴──────────────┴──────────────┴───────┴──────────────┘\n",
      "\n",
      "✓ Saved to: demo_data_downloads/wikipedia/wikipedia.parquet\n",
      "\n",
      "============================================================\n",
      "✓ Download complete!\n",
      "============================================================\n",
      "\n",
      "Output file: demo_data_downloads/wikipedia/wikipedia.parquet\n",
      "\n",
      "Next step: Process with prepare_corpus.py\n",
      "Example:\n",
      "  python bin/prepare_corpus.py \\\n",
      "    --input demo_data_downloads/wikipedia/wikipedia.parquet \\\n",
      "    --output data/processed.parquet \\\n",
      "    --max-grade 8\n",
      "\n",
      "✓ Wikipedia download complete!\n",
      "File: demo_data_downloads/wikipedia/wikipedia.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download Wikipedia data\n",
    "# \n",
    "# This will:\n",
    "# 1. Download Simple Wikipedia dump (~500MB)\n",
    "# 2. Extract the XML file\n",
    "# 3. Use WikiExtractor to parse articles\n",
    "# 4. Convert to parquet format\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/download_data.py\",\n",
    "    \"--source\", \"wikipedia\",\n",
    "    \"--output-dir\", str(data_dir / \"wikipedia\"),\n",
    "    \"--date\", \"20251020\"  # You can change the date\n",
    "], capture_output=False)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ Wikipedia download complete!\")\n",
    "    wiki_file = data_dir / \"wikipedia\" / \"wikipedia.parquet\"\n",
    "    print(f\"File: {wiki_file}\")\n",
    "else:\n",
    "    print(f\"\\n Download failed with code {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Download FineWeb-edu Data\n",
    "\n",
    "Downloads pre-filtered educational web content from HuggingFace.\n",
    "\n",
    "**Requirements**: None (uses Python's `requests` library)\n",
    "\n",
    "**Size**: ~100MB per file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FineWeb-edu data \n",
    "# This downloads educational web content from HuggingFace\n",
    "# Each file is ~100MB and contains pre-filtered educational text\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Download 2 files as an example (you can increase --num-files)\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/download_data.py\",\n",
    "    \"--source\", \"fineweb\",\n",
    "    \"--output-dir\", str(data_dir / \"fineweb\"),\n",
    "    \"--num-files\", \"1\",  # Download 2 files\n",
    "    \"--start-index\", \"0\"  # Start from file 0\n",
    "], capture_output=False)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ FineWeb download complete!\")\n",
    "    fineweb_file = data_dir / \"fineweb\" / \"fineweb_combined.parquet\"\n",
    "    print(f\"File: {fineweb_file}\")\n",
    "else:\n",
    "    print(f\"\\n Download failed with code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "tiny_corpus_prep - Data Download Tool (Step 0)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Source: fineweb\n",
      "  Output directory: demo_data_downloads/fineweb\n",
      "  Number of files: 1\n",
      "  Start index: 0\n",
      "\n",
      "\n",
      "=== Downloading FineWeb-edu ===\n",
      "Downloading 1 file(s) starting from index 0\n",
      "\n",
      "============================================================\n",
      "Starting download: FineWeb 000_00000.parquet\n",
      "File size: 2053.1 MB\n",
      "============================================================\n",
      "\n",
      "Note: Running in non-interactive mode, showing periodic updates...\n",
      "Downloaded: 10.0 MB / 2053.1 MB (0.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "⚠️  Download interrupted by user\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1943, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1901, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_43104/1747368894.py\", line 9, in <cell line: 9>\n",
      "    result = subprocess.run([\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 505, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1146, in communicate\n",
      "    self.wait()\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1222, in wait\n",
      "    self._wait(timeout=sigint_timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1937, in _wait\n",
      "    time.sleep(delay)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 869, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1943, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1901, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_43104/1747368894.py\", line 9, in <cell line: 9>\n",
      "    result = subprocess.run([\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 505, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1146, in communicate\n",
      "    self.wait()\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1222, in wait\n",
      "    self._wait(timeout=sigint_timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1937, in _wait\n",
      "    time.sleep(delay)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 869, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1943, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1901, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_43104/1747368894.py\", line 9, in <cell line: 9>\n",
      "    result = subprocess.run([\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 505, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1146, in communicate\n",
      "    self.wait()\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1222, in wait\n",
      "    self._wait(timeout=sigint_timeout)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/subprocess.py\", line 1937, in _wait\n",
      "    time.sleep(delay)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/inspect.py\", line 869, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 710\n"
     ]
    }
   ],
   "source": [
    "#df_original = pl.read_parquet('demo_data_downloads/wikipedia/wikipedia.parquet')\n",
    "df_original = pl.read_parquet('demo_data_downloads/fineweb/013_00000.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df_original.sample(10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Normalization <a name=\"normalization\"></a>\n",
    "\n",
    "Text normalization includes lowercasing and cleaning up punctuation to create more consistent text for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>text</th><th>id</th><th>dump</th><th>url</th><th>file_path</th><th>language</th><th>language_score</th><th>token_count</th><th>score</th><th>int_score</th><th>text_normalized</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;There are two worlds in Englis…</td><td>&quot;&lt;urn:uuid:8296158a-1667-4aca-8…</td><td>&quot;CC-MAIN-2019-51&quot;</td><td>&quot;https://learnrealenglish.com/e…</td><td>&quot;s3://commoncrawl/crawl-data/CC…</td><td>&quot;en&quot;</td><td>0.943194</td><td>490</td><td>2.703125</td><td>3</td><td>&quot;there are two worlds in englis…</td></tr><tr><td>&quot;ABOUT GOAT KINGDOM\n",
       "SPOTLIGHT M…</td><td>&quot;&lt;urn:uuid:be4b4eec-de22-48f3-9…</td><td>&quot;CC-MAIN-2019-51&quot;</td><td>&quot;http://goatkingdom.tripod.com/…</td><td>&quot;s3://commoncrawl/crawl-data/CC…</td><td>&quot;en&quot;</td><td>0.938428</td><td>3841</td><td>2.71875</td><td>3</td><td>&quot;about goat kingdom\n",
       "spotlight m…</td></tr><tr><td>&quot;Stress Medications - What Medi…</td><td>&quot;&lt;urn:uuid:316a3240-6e86-4122-a…</td><td>&quot;CC-MAIN-2020-05&quot;</td><td>&quot;https://skodaukmotorsport.com/…</td><td>&quot;s3://commoncrawl/crawl-data/CC…</td><td>&quot;en&quot;</td><td>0.968207</td><td>10382</td><td>3.0625</td><td>3</td><td>&quot;stress medications what medici…</td></tr><tr><td>&quot;Presentation on theme: &quot;States…</td><td>&quot;&lt;urn:uuid:df9c7a84-7cb2-416c-8…</td><td>&quot;CC-MAIN-2017-22&quot;</td><td>&quot;http://slideplayer.com/slide/4…</td><td>&quot;s3://commoncrawl/crawl-data/CC…</td><td>&quot;en&quot;</td><td>0.9011</td><td>489</td><td>3.9375</td><td>4</td><td>&quot;presentation on theme states o…</td></tr><tr><td>&quot;Research seems to be demonstra…</td><td>&quot;&lt;urn:uuid:a7d5d003-ee14-476a-b…</td><td>&quot;CC-MAIN-2020-05&quot;</td><td>&quot;https://elrambo.wordpress.com/…</td><td>&quot;s3://commoncrawl/crawl-data/CC…</td><td>&quot;en&quot;</td><td>0.949894</td><td>525</td><td>3.3125</td><td>3</td><td>&quot;research seems to be demonstra…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬──────────┬───────────┬───────────┐\n",
       "│ text      ┆ id        ┆ dump      ┆ url       ┆ … ┆ token_cou ┆ score    ┆ int_score ┆ text_norm │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ nt        ┆ ---      ┆ ---       ┆ alized    │\n",
       "│ str       ┆ str       ┆ str       ┆ str       ┆   ┆ ---       ┆ f64      ┆ i64       ┆ ---       │\n",
       "│           ┆           ┆           ┆           ┆   ┆ i64       ┆          ┆           ┆ str       │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪══════════╪═══════════╪═══════════╡\n",
       "│ There are ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://l ┆ … ┆ 490       ┆ 2.703125 ┆ 3         ┆ there are │\n",
       "│ two       ┆ :8296158a ┆ 019-51    ┆ earnreale ┆   ┆           ┆          ┆           ┆ two       │\n",
       "│ worlds in ┆ -1667-4ac ┆           ┆ nglish.co ┆   ┆           ┆          ┆           ┆ worlds in │\n",
       "│ Englis…   ┆ a-8…      ┆           ┆ m/e…      ┆   ┆           ┆          ┆           ┆ englis…   │\n",
       "│ ABOUT     ┆ <urn:uuid ┆ CC-MAIN-2 ┆ http://go ┆ … ┆ 3841      ┆ 2.71875  ┆ 3         ┆ about     │\n",
       "│ GOAT      ┆ :be4b4eec ┆ 019-51    ┆ atkingdom ┆   ┆           ┆          ┆           ┆ goat      │\n",
       "│ KINGDOM   ┆ -de22-48f ┆           ┆ .tripod.c ┆   ┆           ┆          ┆           ┆ kingdom   │\n",
       "│ SPOTLIGHT ┆ 3-9…      ┆           ┆ om/…      ┆   ┆           ┆          ┆           ┆ spotlight │\n",
       "│ M…        ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆ m…        │\n",
       "│ Stress    ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://s ┆ … ┆ 10382     ┆ 3.0625   ┆ 3         ┆ stress    │\n",
       "│ Medicatio ┆ :316a3240 ┆ 020-05    ┆ kodaukmot ┆   ┆           ┆          ┆           ┆ medicatio │\n",
       "│ ns - What ┆ -6e86-412 ┆           ┆ orsport.c ┆   ┆           ┆          ┆           ┆ ns what   │\n",
       "│ Medi…     ┆ 2-a…      ┆           ┆ om/…      ┆   ┆           ┆          ┆           ┆ medici…   │\n",
       "│ Presentat ┆ <urn:uuid ┆ CC-MAIN-2 ┆ http://sl ┆ … ┆ 489       ┆ 3.9375   ┆ 4         ┆ presentat │\n",
       "│ ion on    ┆ :df9c7a84 ┆ 017-22    ┆ ideplayer ┆   ┆           ┆          ┆           ┆ ion on    │\n",
       "│ theme:    ┆ -7cb2-416 ┆           ┆ .com/slid ┆   ┆           ┆          ┆           ┆ theme     │\n",
       "│ \"States…  ┆ c-8…      ┆           ┆ e/4…      ┆   ┆           ┆          ┆           ┆ states o… │\n",
       "│ Research  ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://e ┆ … ┆ 525       ┆ 3.3125   ┆ 3         ┆ research  │\n",
       "│ seems to  ┆ :a7d5d003 ┆ 020-05    ┆ lrambo.wo ┆   ┆           ┆          ┆           ┆ seems to  │\n",
       "│ be demons ┆ -ee14-476 ┆           ┆ rdpress.c ┆   ┆           ┆          ┆           ┆ be demons │\n",
       "│ tra…      ┆ a-b…      ┆           ┆ om/…      ┆   ┆           ┆          ┆           ┆ tra…      │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴──────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Apply to entire dataframe\n",
    "df_normalized = df_original.with_columns(\n",
    "    pl.col(\"text\").map_elements(normalize_text, return_dtype=pl.Utf8).alias(\"text_normalized\")\n",
    ")\n",
    "\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Readability Filtering <a name=\"readability\"></a>\n",
    "\n",
    "The library uses the Flesch-Kincaid grade level to filter text by reading difficulty. This is useful for creating training data appropriate for specific education levels.\n",
    "# Flesch-Kincaid Grade Level\n",
    "\n",
    "The **Flesch-Kincaid Grade Level** is a readability test that indicates the U.S. school grade level required to understand a piece of text. It was developed by **Rudolf Flesch** and **J. Peter Kincaid** for the U.S. Navy in **1975**.\n",
    "\n",
    "## Formula\n",
    "\n",
    "The score is based on two key factors: sentence length and word complexity.\n",
    "\n",
    "**Grade Level = a × (total words / total sentences) + b × (total syllables / total words) - c**\n",
    "\n",
    "### Components\n",
    "\n",
    "- **Average Sentence Length** = total words / total sentences  \n",
    "  Longer sentences increase complexity.\n",
    "\n",
    "- **Average Syllables per Word** = total syllables / total words  \n",
    "  Words with more syllables indicate higher difficulty.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "| Score | Reading Level | Example Audience            |\n",
    "|-------|----------------|------------------------------|\n",
    "| 0–5   | Elementary     | 5th grade and below          |\n",
    "| 6–8   | Middle School  | 6th–8th grade                |\n",
    "| 9–12  | High School    | 9th–12th grade               |\n",
    "| 13–16 | College        | College undergraduate        |\n",
    "| 17+   | Graduate       | Graduate school and above    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original rows: 10000\n",
      "Filtered rows: 957\n",
      "Removed: 9043 rows\n",
      "\n",
      "Remaining texts:\n",
      "shape: (957, 10)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
      "│ text      ┆ id        ┆ dump      ┆ url       ┆ … ┆ language_ ┆ token_cou ┆ score    ┆ int_score │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ score     ┆ nt        ┆ ---      ┆ ---       │\n",
      "│ str       ┆ str       ┆ str       ┆ str       ┆   ┆ ---       ┆ ---       ┆ f64      ┆ i64       │\n",
      "│           ┆           ┆           ┆           ┆   ┆ f64       ┆ i64       ┆          ┆           │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
      "│ Stress    ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://s ┆ … ┆ 0.968207  ┆ 10382     ┆ 3.0625   ┆ 3         │\n",
      "│ Medicatio ┆ :316a3240 ┆ 020-05    ┆ kodaukmot ┆   ┆           ┆           ┆          ┆           │\n",
      "│ ns - What ┆ -6e86-412 ┆           ┆ orsport.c ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Medi…     ┆ 2-a…      ┆           ┆ om/…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Muhly     ┆ <urn:uuid ┆ CC-MAIN-2 ┆ http://ww ┆ … ┆ 0.925658  ┆ 311       ┆ 3.359375 ┆ 3         │\n",
      "│ grass (Mu ┆ :95bb961d ┆ 017-22    ┆ w.gardeng ┆   ┆           ┆           ┆          ┆           │\n",
      "│ hlenbergi ┆ -590b-4a7 ┆           ┆ uides.com ┆   ┆           ┆           ┆          ┆           │\n",
      "│ a spec…   ┆ d-b…      ┆           ┆ /82…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ A Graduat ┆ <urn:uuid ┆ CC-MAIN-2 ┆ http://de ┆ … ┆ 0.966179  ┆ 630       ┆ 2.921875 ┆ 3         │\n",
      "│ e's Dream ┆ :88d6b2a2 ┆ 020-05    ┆ ryckerede ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Job:      ┆ -c6be-4af ┆           ┆ sign.com/ ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Helpin…   ┆ 5-a…      ┆           ┆ ?H0…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Please    ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://w ┆ … ┆ 0.968083  ┆ 3215      ┆ 2.59375  ┆ 3         │\n",
      "│ subscribe ┆ :28871062 ┆ 019-51    ┆ ww.bringy ┆   ┆           ┆           ┆          ┆           │\n",
      "│ to watch  ┆ -4be6-4ae ┆           ┆ ourownlap ┆   ┆           ┆           ┆          ┆           │\n",
      "│ this…     ┆ a-9…      ┆           ┆ top…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ You've    ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://w ┆ … ┆ 0.88849   ┆ 369       ┆ 2.71875  ┆ 3         │\n",
      "│ spent a   ┆ :b67ff567 ┆ 023-14    ┆ ww.techwa ┆   ┆           ┆           ┆          ┆           │\n",
      "│ great     ┆ -466d-4a1 ┆           ┆ lla.com/a ┆   ┆           ┆           ┆          ┆           │\n",
      "│ deal of   ┆ e-9…      ┆           ┆ rti…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ t…        ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …        ┆ …         │\n",
      "│ Earthquak ┆ <urn:uuid ┆ CC-MAIN-2 ┆ http://la ┆ … ┆ 0.97832   ┆ 646       ┆ 2.578125 ┆ 3         │\n",
      "│ e swarm   ┆ :b366f7a9 ┆ 017-26    ┆ timesblog ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 'classic' ┆ -efd7-402 ┆           ┆ s.latimes ┆   ┆           ┆           ┆          ┆           │\n",
      "│ act…      ┆ d-8…      ┆           ┆ .co…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ 5.1. List ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://w ┆ … ┆ 0.871275  ┆ 401       ┆ 3.125    ┆ 3         │\n",
      "│ and       ┆ :091b80e1 ┆ 023-14    ┆ ritingfor ┆   ┆           ┆           ┆          ┆           │\n",
      "│ briefly   ┆ -545f-453 ┆           ┆ grades.co ┆   ┆           ┆           ┆          ┆           │\n",
      "│ define i… ┆ 3-8…      ┆           ┆ m/5…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ The tale  ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://e ┆ … ┆ 0.983607  ┆ 889       ┆ 2.96875  ┆ 3         │\n",
      "│ of the    ┆ :11ccd6bb ┆ 023-06    ┆ astcoasts ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Dungarvon ┆ -949f-4ba ┆           ┆ langherit ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Whoo…     ┆ 7-b…      ┆           ┆ age…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ ‘How’s    ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://w ┆ … ┆ 0.958488  ┆ 585       ┆ 3.09375  ┆ 3         │\n",
      "│ your baby ┆ :3d92d166 ┆ 023-06    ┆ ww.apta-a ┆   ┆           ┆           ┆          ┆           │\n",
      "│ growing?’ ┆ -2147-462 ┆           ┆ dvice.com ┆   ┆           ┆           ┆          ┆           │\n",
      "│ is …      ┆ 4-a…      ┆           ┆ /pr…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Which     ┆ <urn:uuid ┆ CC-MAIN-2 ┆ https://w ┆ … ┆ 0.913717  ┆ 947       ┆ 4.28125  ┆ 4         │\n",
      "│ number is ┆ :bbb3f613 ┆ 023-14    ┆ hich.wiki ┆   ┆           ┆           ┆          ┆           │\n",
      "│ greater   ┆ -b5b4-482 ┆           ┆ /tutorial ┆   ┆           ┆           ┆          ┆           │\n",
      "│ or        ┆ 8-b…      ┆           ┆ /wh…      ┆   ┆           ┆           ┆          ┆           │\n",
      "│ Wha…      ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆           │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "df_filtered = filter_by_readability(df_original, max_grade=8.0)\n",
    "\n",
    "print(f\"\\nOriginal rows: {len(df_original)}\")\n",
    "print(f\"Filtered rows: {len(df_filtered)}\")\n",
    "print(f\"Removed: {len(df_original) - len(df_filtered)} rows\")\n",
    "print(\"\\nRemaining texts:\")\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building CEFR-Based Synonym Dictionaries <a name=\"cefr-synonyms\"></a>\n",
    "\n",
    "For more vocabulary simplification, you can build synonym dictionaries based on CEFR (Common European Framework of Reference for Languages) levels. This replaces difficult words (B2, C1, C2) with easier synonyms (A1, A2).\n",
    "\n",
    "The library includes a script that uses WordNet and CEFR word lists to automatically generate these mappings.\n",
    "\n",
    "Dataset from https://www.kaggle.com/datasets/nezahatkk/10-000-english-words-cerf-labelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CEFR synonym dictionary built successfully!\n",
      "\n",
      "Output files:\n",
      "  - cefr_synonyms/synonyms.json  (for use in pipeline)\n",
      "  - cefr_synonyms/synonyms.csv   (detailed mapping)\n",
      "  - cefr_synonyms/unmapped.txt   (words without mappings)\n",
      "  - cefr_synonyms/build_stats.txt (statistics)\n"
     ]
    }
   ],
   "source": [
    "# Build a CEFR-based synonym dictionary\n",
    "#\n",
    "# This requires a CEFR wordlist CSV file. The library looks for it in common locations\n",
    "# or you can provide your own.\n",
    "#\n",
    "# The script will:\n",
    "# 1. Identify difficult words (B2, C1, C2 levels)\n",
    "# 2. Find easier synonyms from A1, A2 levels using WordNet\n",
    "# 3. Generate a JSON mapping file\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Note: You'll need a CEFR wordlist CSV file\n",
    "# The script can use common sources like Oxford 5000 or Cambridge wordlists\n",
    "# See CEFR_SYNONYMS.md for more information\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/build_synmap_from_cefr.py\",\n",
    "    \"--cefr_csv\", \"data/ENGLISH_CERF_WORDS.csv\",  # Provide your CEFR wordlist\n",
    "    \"--out_dir\", \"cefr_synonyms\",\n",
    "    \"--easy_levels\", \"A1,A2\",\n",
    "    \"--difficult_levels\", \"B2,C1,C2\"\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ CEFR synonym dictionary built successfully!\")\n",
    "    print(\"\\nOutput files:\")\n",
    "    print(\"  - cefr_synonyms/synonyms.json  (for use in pipeline)\")\n",
    "    print(\"  - cefr_synonyms/synonyms.csv   (detailed mapping)\")\n",
    "    print(\"  - cefr_synonyms/unmapped.txt   (words without mappings)\")\n",
    "    print(\"  - cefr_synonyms/build_stats.txt (statistics)\")\n",
    "else:\n",
    "    print(f\"✗ Build failed: {result.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CEFR Synonym Dictionary\n",
    "\n",
    "Once you've built a CEFR synonym dictionary, you can use it in your processing pipeline for intelligent vocabulary simplification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEFR-based vocabulary simplification:\n",
      "======================================================================\n",
      "\\nOriginal:   The physician utilized sophisticated equipment.\n",
      "Simplified: The physician utilized advanced equipment.\n",
      "\\nOriginal:   They commenced the endeavor immediately.\n",
      "Simplified: They commenced the endeavor immediately.\n",
      "\\nOriginal:   The automobile accelerated rapidly.\n",
      "Simplified: The automobile accelerated rapidly.\n",
      "\\n======================================================================\n",
      "Using CEFR synonyms in a pipeline:\n",
      "Starting pipeline with 10000 rows...\n",
      "Normalizing text...\n",
      "After normalization: 7083 rows\n",
      "Applying synonym mapping...\n",
      "Deduplicating...\n",
      "Removed 2 duplicate rows, 7081 remaining\n",
      "Pipeline complete! Final: 7081 rows\n",
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ text                            │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ john kevin stitt born december… │\n",
      "│ mebane is a city located mostl… │\n",
      "│ man of steel may refer to       │\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Example: Using a CEFR synonym dictionary\n",
    "# (This assumes you've built one using the script above)\n",
    "\n",
    "cefr_synonyms_file = \"cefr_synonyms/synonyms.json\"\n",
    "\n",
    "\n",
    "# Load and use the CEFR synonyms\n",
    "cefr_mapper = SynonymMapper.from_json(cefr_synonyms_file)\n",
    "\n",
    "# Test on some complex texts\n",
    "complex_texts = [\n",
    "    \"The physician utilized sophisticated equipment.\",\n",
    "    \"They commenced the endeavor immediately.\",\n",
    "    \"The automobile accelerated rapidly.\"\n",
    "]\n",
    "\n",
    "print(\"CEFR-based vocabulary simplification:\")\n",
    "print(\"=\"*70)\n",
    "for text in complex_texts:\n",
    "    simplified = cefr_mapper.simplify_line(text)\n",
    "    print(f\"\\\\nOriginal:   {text}\")\n",
    "    print(f\"Simplified: {simplified}\")\n",
    "\n",
    "# Use in a pipeline\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"Using CEFR synonyms in a pipeline:\")\n",
    "\n",
    "pipeline = DataPipeline().add_synonym_mapper(mapping_path=cefr_synonyms_file)\n",
    "df_cefr_simplified = pipeline.process(df_original)\n",
    "print(df_cefr_simplified.select([\"text\"]).head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Annotations <a name=\"custom\"></a>\n",
    "\n",
    "Add custom metadata to your text using annotation functions. This can include word counts, sentiment, complexity scores, or any other features you want to track.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text:\n",
      "  The cat sat on the mat. It was a sunny day.\n",
      "\n",
      "Extracted features:\n",
      "  word_count: 11\n",
      "  char_count: 43\n",
      "  sentence_count: 2\n",
      "  avg_word_length: 3.0\n",
      "  has_numbers: False\n",
      "\n",
      "======================================================================\n",
      "Applying custom annotations to all texts:\n",
      "======================================================================\n",
      "Starting pipeline with 10000 rows...\n",
      "Applying annotator 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating: 100%|███████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 50228.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline complete! Final: 10000 rows\n",
      "\n",
      "Annotated dataframe (first 5 rows):\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ filename  ┆ title     ┆ text      ┆ number_of ┆ … ┆ char_coun ┆ sentence_ ┆ avg_word_ ┆ has_numb │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ _characte ┆   ┆ t         ┆ count     ┆ length    ┆ ers      │\n",
      "│ str       ┆ str       ┆ str       ┆ rs        ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│           ┆           ┆           ┆ ---       ┆   ┆ i64       ┆ i64       ┆ f64       ┆ bool     │\n",
      "│           ┆           ┆           ┆ i64       ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ demo_data ┆ Emmy      ┆ The Emmy  ┆ 785       ┆ … ┆ 785       ┆ 8         ┆ 4.8       ┆ true     │\n",
      "│ _download ┆ Award     ┆ Awards    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆ are       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆ United    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆           ┆ Sta…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ demo_data ┆ Dooly (ch ┆           ┆ 0         ┆ … ┆ 0         ┆ 1         ┆ 0.0       ┆ false    │\n",
      "│ _download ┆ aracter)  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ demo_data ┆ My Ex and ┆ My Ex and ┆ 238       ┆ … ┆ 238       ┆ 2         ┆ 5.1       ┆ true     │\n",
      "│ _download ┆ Whys      ┆ Whys is a ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆ 2017      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆ Filip…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ demo_data ┆ Victoria, ┆ Victoria, ┆ 154       ┆ … ┆ 154       ┆ 1         ┆ 4.7       ┆ false    │\n",
      "│ _download ┆ Labuan    ┆ or        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆ locally   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆ known as  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆           ┆ …         ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ demo_data ┆ Dobrogea  ┆           ┆ 0         ┆ … ┆ 0         ┆ 1         ┆ 0.0       ┆ false    │\n",
      "│ _download ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Define custom annotation function\n",
    "def text_features(text: str) -> dict:\n",
    "    \"\"\"Extract custom features from text.\"\"\"\n",
    "    words = text.split()\n",
    "    sentences = text.count(\".\") + text.count(\"!\") + text.count(\"?\")\n",
    "    \n",
    "    return {\n",
    "        \"word_count\": len(words),\n",
    "        \"char_count\": len(text),\n",
    "        \"sentence_count\": max(1, sentences),  # At least 1\n",
    "        \"avg_word_length\": round(sum(len(w) for w in words) / len(words), 1) if words else 0,\n",
    "        \"has_numbers\": any(c.isdigit() for c in text),\n",
    "    }\n",
    "\n",
    "# Test on a sample text\n",
    "sample = \"The cat sat on the mat. It was a sunny day.\"\n",
    "features = text_features(sample)\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(f\"  {sample}\")\n",
    "print(\"\\nExtracted features:\")\n",
    "for key, value in features.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create annotator and apply to dataframe\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Applying custom annotations to all texts:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "annotator = CustomFunctionAnnotator(text_features)\n",
    "# Turn off normalization and dedup for this example to keep original text\n",
    "pipeline = DataPipeline(text_column=\"text\", normalize=False, dedup=False).add_annotator(annotator)\n",
    "df_annotated = pipeline.process(df_original)\n",
    "\n",
    "print(\"\\nAnnotated dataframe (first 5 rows):\")\n",
    "print(df_annotated.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Gemini for AI-Powered Annotations <a name=\"gemini\"></a>\n",
    "\n",
    "The library includes a built-in Google Gemini annotator that can automatically classify text by topic and education level using AI. This is more sophisticated than keyword-based classification.\n",
    "\n",
    "**Requirements**:\n",
    "- Install annotator dependencies: `uv pip install -e \".[annotators]\"`\n",
    "- Google API key: Set `GOOGLE_API_KEY` environment variable\n",
    "\n",
    "**What it does**:\n",
    "- Classifies text into 20+ topic categories (Mathematics, Computer Science, Life Sciences, etc.)\n",
    "- Determines education level (primary school, middle school, high school, university, PhD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df_original.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini annotator initialized\n",
      "\\nProcessing texts with Gemini (this may take a minute)...\n",
      "Starting pipeline with 100 rows...\n",
      "Applying annotator 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating: 100%|██████████████████████████████████████████████████████████████████| 100/100 [00:39<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline complete! Final: 100 rows\n",
      "\\n✓ Gemini annotation complete!\n",
      "\\nColumns: ['text', 'id', 'dump', 'url', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score', 'topic', 'education']\n",
      "\\nSample results:\n",
      "shape: (5, 3)\n",
      "┌─────────────────────────────────┬───────────────────────┬───────────────────┐\n",
      "│ text                            ┆ topic                 ┆ education         │\n",
      "│ ---                             ┆ ---                   ┆ ---               │\n",
      "│ str                             ┆ str                   ┆ str               │\n",
      "╞═════════════════════════════════╪═══════════════════════╪═══════════════════╡\n",
      "│ How Australia Day is Dividing … ┆ Social Sciences       ┆ high school       │\n",
      "│ October 1997 bird's-eye view o… ┆ History & Archaeology ┆ high school       │\n",
      "│ WINONA, Minn. (AP) -- They are… ┆ Life Sciences         ┆ high school       │\n",
      "│ The authors of the new study, … ┆ Health & Medicine     ┆ university degree │\n",
      "│ The Church has been celebratin… ┆ Social Sciences       ┆ high school       │\n",
      "└─────────────────────────────────┴───────────────────────┴───────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Gemini Annotator\n",
    "#\n",
    "# This uses Google's Gemini AI to classify text by topic and education level\n",
    "# It's more accurate than keyword matching but requires an API key\n",
    "#\n",
    "# Setup:\n",
    "# 1. Get a Google API key from https://makersuite.google.com/app/apikey\n",
    "# 2. Set it as environment variable: export GOOGLE_API_KEY=\"your_key\"\n",
    "#    Or create a .env file with: GOOGLE_API_KEY=your_key\n",
    "# 3. Install dependencies: uv pip install -e \".[annotators]\"\n",
    "#\n",
    "\n",
    "from tiny_corpus_prep import GeminiAnnotator\n",
    "import os\n",
    "# \n",
    "# # Check if API key is available\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"MY_API_KEY\")\n",
    "# \n",
    "if api_key:\n",
    "    try:\n",
    "        # Initialize Gemini annotator\n",
    "        gemini = GeminiAnnotator(\n",
    "            model_name=\"gemini-2.5-flash-lite\",  # Fast and cheap model\n",
    "            temperature=0.1,  # Low temperature for consistent results\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Gemini annotator initialized\")\n",
    "        \n",
    "        # Create pipeline with Gemini\n",
    "        gemini_pipeline = (\n",
    "            DataPipeline(text_column=\"text\", normalize=False, dedup=False)\n",
    "            .add_annotator(gemini)\n",
    "        )\n",
    "        \n",
    "        # Process our sample data (this will call the Gemini API)\n",
    "        print(\"\\\\nProcessing texts with Gemini (this may take a minute)...\")\n",
    "        df_gemini = gemini_pipeline.process(df_original)\n",
    "        \n",
    "        print(\"\\\\n✓ Gemini annotation complete!\")\n",
    "        print(f\"\\\\nColumns: {df_gemini.columns}\")\n",
    "        \n",
    "        # Show results\n",
    "        print(\"\\\\nSample results:\")\n",
    "        print(df_gemini.select([\"text\", \"topic\", \"education\"]).head(5))\n",
    "        \n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"✗ Import error: {e}\")\n",
    "        print(\"\\\\nPlease install annotation dependencies:\")\n",
    "        print(\"  uv pip install -e '.[annotators]'\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "else:\n",
    "    print(\" GOOGLE_API_KEY not found in environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API Cost and Performance\n",
    "\n",
    "**Cost**: Gemini 2.5 Flash Lite is very affordable:\n",
    "- around 0.20 dollar for 1 million input tokens\n",
    "- Processing 10,000 texts (~50 words each) costs approximately $0.05\n",
    "\n",
    "**Performance**:\n",
    "- Processes ~5 texts per second\n",
    "- Can handle texts up to 15,000 characters\n",
    "\n",
    "**Tips**:\n",
    "- Use `gemini-2.5-flash-lite` for cost-effective processing\n",
    "- Set `temperature=0.1` for consistent results\n",
    "- Process in batches for large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline Example <a name=\"pipeline\"></a>\n",
    "\n",
    "Now let's combine multiple processing steps using the high-level `process_corpus` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus with:\n",
      "  - Text normalization\n",
      "  - Readability filter (max grade 10)\n",
      "  - Keyword filter (science, math, programming)\n",
      "  - Synonym mapping\n",
      "  - Deduplication\n",
      "\n",
      "======================================================================\n",
      "Reading input from: ../.cache/nanochat/base_data/shard_00000.parquet\n",
      "Loaded 53248 rows with columns: ['text']\n",
      "Starting pipeline with 53248 rows...\n",
      "Normalizing text...\n"
     ]
    }
   ],
   "source": [
    "# Process corpus with multiple filters\n",
    "print(\"Processing corpus with:\")\n",
    "print(\"  - Text normalization\")\n",
    "print(\"  - Readability filter (max grade 10)\")\n",
    "print(\"  - Keyword filter (science, math, programming)\")\n",
    "print(\"  - Synonym mapping\")\n",
    "print(\"  - Deduplication\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "stats = process_corpus(\n",
    "    input_path='./demo_data_downloads/wikipedia/wikipedia.parquet',\n",
    "    output_path=\"demo_processed.parquet\",\n",
    "    normalize=True,\n",
    "    max_grade=10.0,\n",
    "    keywords=[\"science\" \"math\", \"programming\", \"learning\", \"biology\", \"physics\"],\n",
    "    synonyms_map_path=\"./cefr_synonyms/synonyms.json\",\n",
    "    dedup=True,\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"\\nTotal rows in output: {stats['total_rows']}\")\n",
    "print(f\"Total columns: {stats['total_columns']}\")\n",
    "print(f\"Columns: {stats['columns']}\")\n",
    "\n",
    "# Load and display processed data\n",
    "df_processed = pl.read_parquet(\"demo_processed.parquet\")\n",
    "print(\"\\nProcessed data:\")\n",
    "print(df_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
