{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook showcases the key features and processing steps of the **tiny_corpus_prep** library, a Polars-based corpus preparation tool for training tiny GPT-like models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation <a name=\"setup\"></a>\n",
    "\n",
    "First, let's import the necessary modules and verify the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import polars as pl\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import tiny_corpus_prep components (main API)\n",
    "from tiny_corpus_prep import (\n",
    "    process_corpus,\n",
    "    DataPipeline,\n",
    "    read_parquet,\n",
    "    write_parquet_with_stats,\n",
    "    filter_by_readability,\n",
    "    filter_by_keywords,\n",
    "    is_middle_school_level,\n",
    "    CustomFunctionAnnotator,\n",
    "    generate_stats,\n",
    ")\n",
    "\n",
    "# Import internal utilities (not part of public API but useful for demos)\n",
    "from tiny_corpus_prep.normalize import normalize_text\n",
    "from tiny_corpus_prep.synonyms import SynonymMapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data <a name=\"download\"></a>\n",
    "\n",
    "Before processing, you can download real datasets from Wikipedia or FineWeb-edu. \n",
    "\n",
    "### Data Sources:\n",
    "- **Wikipedia**: Simple English Wikipedia - great for general knowledge\n",
    "- **FineWeb-edu**: Pre-filtered educational web content from HuggingFace\n",
    "\n",
    "**Note**: These downloads can be large (100MB - 2GB+) and may take several minutes. The cells below are commented out by default - uncomment to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data directory: demo_data_downloads\n",
      "Full path: /home/gillus/tiny_corpus_prep_lib/demo_data_downloads\n"
     ]
    }
   ],
   "source": [
    "# Create a data directory for downloads\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"demo_data_downloads\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created data directory: {data_dir}\")\n",
    "print(f\"Full path: {data_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Download Wikipedia Data\n",
    "\n",
    "Downloads Simple English Wikipedia, extracts articles, and converts to parquet format.\n",
    "\n",
    "**Requirements**: `bzip2` (system), `wikiextractor` (Python package)\n",
    "\n",
    "**Size**: ~500MB compressed, expands to ~2GB\n",
    "\n",
    "Wikipedia content is released under the Creative Commons Attribution-ShareAlike (CC BY-SA 4.0) license. This means that any model trained on Wikipedia data, as well as any outputs that constitute derivative works, must comply with CC BY-SA requirements. In practice, you must ensure that attribution is preserved (e.g., acknowledging Wikipedia as a source in documentation), and any redistributed derivative content is shared under the same license. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "tiny_corpus_prep - Data Download Tool (Step 0)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Source: wikipedia\n",
      "  Output directory: demo_data_downloads/wikipedia\n",
      "  Date: 20251020\n",
      "\n",
      "\n",
      "=== Downloading Simple Wikipedia (20251020) ===\n",
      "Downloading from: https://dumps.wikimedia.org/simplewiki/20251020/simplewiki-20251020-pages-articles-multistream.xml.bz2\n",
      "\n",
      "============================================================\n",
      "Starting download: Wikipedia 20251020\n",
      "File size: 346.4 MB\n",
      "============================================================\n",
      "\n",
      "Note: Running in non-interactive mode, showing periodic updates...\n",
      "Downloaded: 10.0 MB / 346.4 MB (2.9%)\n",
      "Downloaded: 20.0 MB / 346.4 MB (5.8%)\n",
      "Downloaded: 30.0 MB / 346.4 MB (8.7%)\n",
      "Downloaded: 40.0 MB / 346.4 MB (11.5%)\n",
      "Downloaded: 50.0 MB / 346.4 MB (14.4%)\n",
      "Downloaded: 60.0 MB / 346.4 MB (17.3%)\n",
      "Downloaded: 70.0 MB / 346.4 MB (20.2%)\n",
      "Downloaded: 80.0 MB / 346.4 MB (23.1%)\n",
      "Downloaded: 90.0 MB / 346.4 MB (26.0%)\n",
      "Downloaded: 100.0 MB / 346.4 MB (28.9%)\n",
      "Downloaded: 110.0 MB / 346.4 MB (31.8%)\n",
      "Downloaded: 120.0 MB / 346.4 MB (34.6%)\n",
      "Downloaded: 130.0 MB / 346.4 MB (37.5%)\n",
      "Downloaded: 140.0 MB / 346.4 MB (40.4%)\n",
      "Downloaded: 150.0 MB / 346.4 MB (43.3%)\n",
      "Downloaded: 160.0 MB / 346.4 MB (46.2%)\n",
      "Downloaded: 170.0 MB / 346.4 MB (49.1%)\n",
      "Downloaded: 180.0 MB / 346.4 MB (52.0%)\n",
      "Downloaded: 190.0 MB / 346.4 MB (54.8%)\n",
      "Downloaded: 200.0 MB / 346.4 MB (57.7%)\n",
      "Downloaded: 210.0 MB / 346.4 MB (60.6%)\n",
      "Downloaded: 220.0 MB / 346.4 MB (63.5%)\n",
      "Downloaded: 230.0 MB / 346.4 MB (66.4%)\n",
      "Downloaded: 240.0 MB / 346.4 MB (69.3%)\n",
      "Downloaded: 250.0 MB / 346.4 MB (72.2%)\n",
      "Downloaded: 260.0 MB / 346.4 MB (75.0%)\n",
      "Downloaded: 270.0 MB / 346.4 MB (77.9%)\n",
      "Downloaded: 280.0 MB / 346.4 MB (80.8%)\n",
      "Downloaded: 290.0 MB / 346.4 MB (83.7%)\n",
      "Downloaded: 300.0 MB / 346.4 MB (86.6%)\n",
      "Downloaded: 310.0 MB / 346.4 MB (89.5%)\n",
      "Downloaded: 320.0 MB / 346.4 MB (92.4%)\n",
      "Downloaded: 330.0 MB / 346.4 MB (95.3%)\n",
      "Downloaded: 340.0 MB / 346.4 MB (98.1%)\n",
      "\n",
      "============================================================\n",
      "✓ Download complete: Wikipedia 20251020\n",
      "Total downloaded: 346.4 MB\n",
      "============================================================\n",
      "\n",
      "\n",
      "Extracting...\n",
      "\n",
      "=== Extracting Wikipedia content ===\n",
      "Running WikiExtractor (output to demo_data_downloads/wikipedia/extracted)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Preprocessing 'demo_data_downloads/wikipedia/simplewiki-20251020-pages-articles-multistream.xml' to collect template definitions: this may take some time.\n",
      "INFO: Preprocessed 100000 pages\n",
      "INFO: Preprocessed 200000 pages\n",
      "INFO: Preprocessed 300000 pages\n",
      "INFO: Preprocessed 400000 pages\n",
      "INFO: Preprocessed 500000 pages\n",
      "INFO: Loaded 43176 templates in 22.8s\n",
      "INFO: Starting page extraction from demo_data_downloads/wikipedia/simplewiki-20251020-pages-articles-multistream.xml.\n",
      "INFO: Using 7 extract processes.\n",
      "INFO: Extracted 100000 articles (3120.5 art/s)\n",
      "INFO: Extracted 200000 articles (3662.0 art/s)\n",
      "INFO: Extracted 300000 articles (3975.8 art/s)\n",
      "INFO: Finished 7-process extraction of 382659 articles in 108.9s (3514.1 art/s)\n",
      "Processing files:   0%|          | 0/246 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Converting to Parquet ===\n",
      "Found 246 JSON files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 246/246 [00:05<00:00, 41.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created DataFrame with 382658 rows\n",
      "shape: (5, 7)\n",
      "┌───────────────┬──────────────┬──────────────┬──────────────┬──────────────┬───────┬──────────────┐\n",
      "│ filename      ┆ title        ┆ text         ┆ number_of_ch ┆ number_of_wo ┆ topic ┆ text_quality │\n",
      "│ ---           ┆ ---          ┆ ---          ┆ aracters     ┆ rds          ┆ ---   ┆ ---          │\n",
      "│ str           ┆ str          ┆ str          ┆ ---          ┆ ---          ┆ str   ┆ i64          │\n",
      "│               ┆              ┆              ┆ i64          ┆ i64          ┆       ┆              │\n",
      "╞═══════════════╪══════════════╪══════════════╪══════════════╪══════════���═══╪═══════╪══════════════╡\n",
      "│ demo_data_dow ┆ Beverly      ┆ Beverly      ┆ 262          ┆ 41           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Hills Madam  ┆ Hills Madam  ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ (also know…  ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ The Light at ┆ The Light at ┆ 269          ┆ 41           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ the Edge of  ┆ the Edge of  ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆ the W…       ┆ the W…       ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ INTLAB       ┆ INTLAB       ┆ 607          ┆ 90           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆              ┆ (INTerval    ┆              ┆              ���       ┆              │\n",
      "│ dia/…         ┆              ┆ LABoratory)  ┆              ┆              ┆       ┆              │\n",
      "│               ┆              ┆ i…           ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Lemon        ┆ Lemon        ┆ 263          ┆ 44           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Popsicle     ┆ Popsicle     ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ (Israeli:    ┆              ┆              ┆       ┆              │\n",
      "│               ┆              ┆ Eskim…       ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Ace Eli and  ┆ Ace Eli and  ┆ 230          ┆ 37           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Rodger of    ┆ Rodger of    ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆ the Skie…    ┆ the Skie…    ┆              ┆              ┆       ┆              │\n",
      "└───────────────┴──────────────┴──────────────┴──────────────┴──────────────┴───────┴──────────────┘\n",
      "\n",
      "✓ Saved to: demo_data_downloads/wikipedia/wikipedia.parquet\n",
      "\n",
      "============================================================\n",
      "✓ Download complete!\n",
      "============================================================\n",
      "\n",
      "Output file: demo_data_downloads/wikipedia/wikipedia.parquet\n",
      "\n",
      "Next step: Process with prepare_corpus.py\n",
      "Example:\n",
      "  python bin/prepare_corpus.py \\\n",
      "    --input demo_data_downloads/wikipedia/wikipedia.parquet \\\n",
      "    --output data/processed.parquet \\\n",
      "    --max-grade 8\n",
      "\n",
      "✓ Wikipedia download complete!\n",
      "File: demo_data_downloads/wikipedia/wikipedia.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download Wikipedia data\n",
    "# \n",
    "# This will:\n",
    "# 1. Download Simple Wikipedia dump (~500MB)\n",
    "# 2. Extract the XML file\n",
    "# 3. Use WikiExtractor to parse articles\n",
    "# 4. Convert to parquet format\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/download_data.py\",\n",
    "    \"--source\", \"wikipedia\",\n",
    "    \"--output-dir\", str(data_dir / \"wikipedia\"),\n",
    "    \"--date\", \"20251020\"  # You can change the date\n",
    "], capture_output=False)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ Wikipedia download complete!\")\n",
    "    wiki_file = data_dir / \"wikipedia\" / \"wikipedia.parquet\"\n",
    "    print(f\"File: {wiki_file}\")\n",
    "else:\n",
    "    print(f\"\\n❌ Download failed with code {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Download FineWeb-edu Data\n",
    "\n",
    "Downloads pre-filtered educational web content from HuggingFace.\n",
    "\n",
    "**Requirements**: None (uses Python's `requests` library)\n",
    "\n",
    "**Size**: ~100MB per file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "tiny_corpus_prep - Data Download Tool (Step 0)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Source: fineweb\n",
      "  Output directory: demo_data_downloads/fineweb\n",
      "  Number of files: 1\n",
      "  Start index: 0\n",
      "\n",
      "\n",
      "=== Downloading FineWeb-edu ===\n",
      "Downloading 1 file(s) starting from index 0\n",
      "\n",
      "============================================================\n",
      "Starting download: FineWeb 000_00000.parquet\n",
      "File size: 2053.1 MB\n",
      "============================================================\n",
      "\n",
      "Note: Running in non-interactive mode, showing periodic updates...\n",
      "Downloaded: 10.0 MB / 2053.1 MB (0.5%)\n",
      "Downloaded: 20.0 MB / 2053.1 MB (1.0%)\n",
      "Downloaded: 30.0 MB / 2053.1 MB (1.5%)\n",
      "Downloaded: 40.0 MB / 2053.1 MB (1.9%)\n",
      "Downloaded: 50.0 MB / 2053.1 MB (2.4%)\n",
      "Downloaded: 60.0 MB / 2053.1 MB (2.9%)\n",
      "Downloaded: 70.0 MB / 2053.1 MB (3.4%)\n",
      "Downloaded: 80.0 MB / 2053.1 MB (3.9%)\n",
      "Downloaded: 90.0 MB / 2053.1 MB (4.4%)\n",
      "Downloaded: 100.0 MB / 2053.1 MB (4.9%)\n",
      "Downloaded: 110.0 MB / 2053.1 MB (5.4%)\n",
      "Downloaded: 120.0 MB / 2053.1 MB (5.8%)\n",
      "Downloaded: 130.0 MB / 2053.1 MB (6.3%)\n",
      "Downloaded: 140.0 MB / 2053.1 MB (6.8%)\n",
      "Downloaded: 150.0 MB / 2053.1 MB (7.3%)\n",
      "Downloaded: 160.0 MB / 2053.1 MB (7.8%)\n",
      "Downloaded: 170.0 MB / 2053.1 MB (8.3%)\n",
      "Downloaded: 180.0 MB / 2053.1 MB (8.8%)\n",
      "Downloaded: 190.0 MB / 2053.1 MB (9.3%)\n",
      "Downloaded: 200.0 MB / 2053.1 MB (9.7%)\n",
      "Downloaded: 210.0 MB / 2053.1 MB (10.2%)\n",
      "Downloaded: 220.0 MB / 2053.1 MB (10.7%)\n",
      "Downloaded: 230.0 MB / 2053.1 MB (11.2%)\n",
      "Downloaded: 240.0 MB / 2053.1 MB (11.7%)\n",
      "Downloaded: 250.0 MB / 2053.1 MB (12.2%)\n",
      "Downloaded: 260.0 MB / 2053.1 MB (12.7%)\n",
      "Downloaded: 270.0 MB / 2053.1 MB (13.2%)\n",
      "Downloaded: 280.0 MB / 2053.1 MB (13.6%)\n",
      "Downloaded: 290.0 MB / 2053.1 MB (14.1%)\n",
      "Downloaded: 300.0 MB / 2053.1 MB (14.6%)\n",
      "Downloaded: 310.0 MB / 2053.1 MB (15.1%)\n",
      "Downloaded: 320.0 MB / 2053.1 MB (15.6%)\n",
      "Downloaded: 330.0 MB / 2053.1 MB (16.1%)\n",
      "Downloaded: 340.0 MB / 2053.1 MB (16.6%)\n",
      "Downloaded: 350.0 MB / 2053.1 MB (17.0%)\n",
      "Downloaded: 360.0 MB / 2053.1 MB (17.5%)\n",
      "Downloaded: 370.0 MB / 2053.1 MB (18.0%)\n",
      "Downloaded: 380.0 MB / 2053.1 MB (18.5%)\n",
      "Downloaded: 390.0 MB / 2053.1 MB (19.0%)\n",
      "Downloaded: 400.0 MB / 2053.1 MB (19.5%)\n",
      "Downloaded: 410.0 MB / 2053.1 MB (20.0%)\n",
      "Downloaded: 420.0 MB / 2053.1 MB (20.5%)\n",
      "Downloaded: 430.0 MB / 2053.1 MB (20.9%)\n",
      "Downloaded: 440.0 MB / 2053.1 MB (21.4%)\n",
      "Downloaded: 450.0 MB / 2053.1 MB (21.9%)\n",
      "Downloaded: 460.0 MB / 2053.1 MB (22.4%)\n",
      "Downloaded: 470.0 MB / 2053.1 MB (22.9%)\n",
      "Downloaded: 480.0 MB / 2053.1 MB (23.4%)\n",
      "Downloaded: 490.0 MB / 2053.1 MB (23.9%)\n",
      "Downloaded: 500.0 MB / 2053.1 MB (24.4%)\n",
      "Downloaded: 510.0 MB / 2053.1 MB (24.8%)\n",
      "Downloaded: 520.0 MB / 2053.1 MB (25.3%)\n",
      "Downloaded: 530.0 MB / 2053.1 MB (25.8%)\n",
      "Downloaded: 540.0 MB / 2053.1 MB (26.3%)\n",
      "Downloaded: 550.0 MB / 2053.1 MB (26.8%)\n",
      "Downloaded: 560.0 MB / 2053.1 MB (27.3%)\n",
      "Downloaded: 570.0 MB / 2053.1 MB (27.8%)\n",
      "Downloaded: 580.0 MB / 2053.1 MB (28.3%)\n",
      "Downloaded: 590.0 MB / 2053.1 MB (28.7%)\n",
      "Downloaded: 600.0 MB / 2053.1 MB (29.2%)\n",
      "Downloaded: 610.0 MB / 2053.1 MB (29.7%)\n",
      "Downloaded: 620.0 MB / 2053.1 MB (30.2%)\n",
      "Downloaded: 630.0 MB / 2053.1 MB (30.7%)\n",
      "Downloaded: 640.0 MB / 2053.1 MB (31.2%)\n",
      "\n",
      "❌ Error: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2548)\n",
      "Error type: SSLError\n",
      "\n",
      "Full traceback:\n",
      "\n",
      "❌ Download failed with code 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/urllib3/response.py\", line 754, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/urllib3/response.py\", line 879, in _raw_read\n",
      "    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/urllib3/response.py\", line 862, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/http/client.py\", line 465, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/ssl.py\", line 1130, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ssl.SSLError: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2548)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/requests/models.py\", line 820, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/urllib3/response.py\", line 1066, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/urllib3/response.py\", line 955, in read\n",
      "    data = self._raw_read(amt)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/urllib3/response.py\", line 878, in _raw_read\n",
      "    with self._error_catcher():\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/urllib3/response.py\", line 765, in _error_catcher\n",
      "    raise SSLError(e) from e\n",
      "urllib3.exceptions.SSLError: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2548)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/tiny_corpus_prep_lib/bin/download_data.py\", line 364, in main\n",
      "    output_file = download_fineweb(args.output_dir, args.num_files, args.start_index)\n",
      "  File \"/home/gillus/tiny_corpus_prep_lib/bin/download_data.py\", line 264, in download_fineweb\n",
      "    download_file_with_progress(file_url, output_file, desc=f\"FineWeb {filename}\")\n",
      "  File \"/home/gillus/tiny_corpus_prep_lib/bin/download_data.py\", line 96, in download_file_with_progress\n",
      "    for chunk in response.iter_content(chunk_size=block_size):\n",
      "  File \"/home/gillus/miniconda3/lib/python3.10/site-packages/requests/models.py\", line 828, in generate\n",
      "    raise RequestsSSLError(e)\n",
      "requests.exceptions.SSLError: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2548)\n"
     ]
    }
   ],
   "source": [
    "# Download FineWeb-edu data \n",
    "# This downloads educational web content from HuggingFace\n",
    "# Each file is ~100MB and contains pre-filtered educational text\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Download 2 files as an example (you can increase --num-files)\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/download_data.py\",\n",
    "    \"--source\", \"fineweb\",\n",
    "    \"--output-dir\", str(data_dir / \"fineweb\"),\n",
    "    \"--num-files\", \"1\",  # Download 2 files\n",
    "    \"--start-index\", \"0\"  # Start from file 0\n",
    "], capture_output=False)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ FineWeb download complete!\")\n",
    "    fineweb_file = data_dir / \"fineweb\" / \"fineweb_combined.parquet\"\n",
    "    print(f\"File: {fineweb_file}\")\n",
    "else:\n",
    "    print(f\"\\n❌ Download failed with code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pl.read_parquet('demo_data_downloads/wikipedia/wikipedia.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10_000, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>filename</th><th>title</th><th>text</th><th>number_of_characters</th><th>number_of_words</th><th>topic</th><th>text_quality</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Sélestat&quot;</td><td>&quot;Sélestat is a commune in north…</td><td>1522</td><td>244</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Kingdom of Asturias&quot;</td><td>&quot;The Kingdom of Asturias (; ) w…</td><td>3628</td><td>602</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Oranienburg&quot;</td><td>&quot;Oranienburg (Slavic languages:…</td><td>698</td><td>115</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Oil and Natural Gas Corporatio…</td><td>&quot;Oil and Natural Gas Corporatio…</td><td>1148</td><td>197</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;ISO 3166&quot;</td><td>&quot;ISO 3166 is a standard made by…</td><td>490</td><td>81</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;New town&quot;</td><td>&quot;A new town, planned community,…</td><td>641</td><td>99</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Gratis versus libre&quot;</td><td>&quot;In English, the word &quot;free&quot; ha…</td><td>190</td><td>32</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Extra virgin olive oil&quot;</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Masahito Suzuki&quot;</td><td>&quot;Masahito Suzuki (born 28 April…</td><td>74</td><td>12</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Beauvois-en-Vermandois&quot;</td><td>&quot;Beauvois-en-Vermandois is a co…</td><td>126</td><td>20</td><td>&quot;N-A&quot;</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10_000, 7)\n",
       "┌───────────────┬──────────────┬──────────────┬──────────────┬──────────────┬───────┬──────────────┐\n",
       "│ filename      ┆ title        ┆ text         ┆ number_of_ch ┆ number_of_wo ┆ topic ┆ text_quality │\n",
       "│ ---           ┆ ---          ┆ ---          ┆ aracters     ┆ rds          ┆ ---   ┆ ---          │\n",
       "│ str           ┆ str          ┆ str          ┆ ---          ┆ ---          ┆ str   ┆ i64          │\n",
       "│               ┆              ┆              ┆ i64          ┆ i64          ┆       ┆              │\n",
       "╞═══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═══════╪══════════════╡\n",
       "│ demo_data_dow ┆ Sélestat     ┆ Sélestat is  ┆ 1522         ┆ 244          ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆ a commune in ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ north…       ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Kingdom of   ┆ The Kingdom  ┆ 3628         ┆ 602          ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ Asturias     ┆ of Asturias  ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ (; ) w…      ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Oranienburg  ┆ Oranienburg  ┆ 698          ┆ 115          ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆ (Slavic      ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ languages:…  ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Oil and      ┆ Oil and      ┆ 1148         ┆ 197          ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ Natural Gas  ┆ Natural Gas  ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆ Corporatio…  ┆ Corporatio…  ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ ISO 3166     ┆ ISO 3166 is  ┆ 490          ┆ 81           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆ a standard   ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ made by…     ┆              ┆              ┆       ┆              │\n",
       "│ …             ┆ …            ┆ …            ┆ …            ┆ …            ┆ …     ┆ …            │\n",
       "│ demo_data_dow ┆ New town     ┆ A new town,  ┆ 641          ┆ 99           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆ planned      ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ community,…  ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Gratis       ┆ In English,  ┆ 190          ┆ 32           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ versus libre ┆ the word     ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ \"free\" ha…   ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Extra virgin ┆              ┆ 0            ┆ 0            ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ olive oil    ┆              ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆              ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Masahito     ┆ Masahito     ┆ 74           ┆ 12           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ Suzuki       ┆ Suzuki (born ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ 28 April…    ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Beauvois-en- ┆ Beauvois-en- ┆ 126          ┆ 20           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ Vermandois   ┆ Vermandois   ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ is a co…     ┆              ┆              ┆       ┆              │\n",
       "└───────────────┴──────────────┴──────────────┴──────────────┴──────────────┴───────┴──────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original = df_original.sample(10000)\n",
    "\n",
    "df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Normalization <a name=\"normalization\"></a>\n",
    "\n",
    "Text normalization includes lowercasing and cleaning up punctuation to create more consistent text for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>filename</th><th>title</th><th>text</th><th>number_of_characters</th><th>number_of_words</th><th>topic</th><th>text_quality</th><th>text_normalized</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Sélestat&quot;</td><td>&quot;Sélestat is a commune in north…</td><td>1522</td><td>244</td><td>&quot;N-A&quot;</td><td>0</td><td>&quot;s lestat is a commune in north…</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Kingdom of Asturias&quot;</td><td>&quot;The Kingdom of Asturias (; ) w…</td><td>3628</td><td>602</td><td>&quot;N-A&quot;</td><td>0</td><td>&quot;the kingdom of asturias was a …</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Oranienburg&quot;</td><td>&quot;Oranienburg (Slavic languages:…</td><td>698</td><td>115</td><td>&quot;N-A&quot;</td><td>0</td><td>&quot;oranienburg slavic languages b…</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Oil and Natural Gas Corporatio…</td><td>&quot;Oil and Natural Gas Corporatio…</td><td>1148</td><td>197</td><td>&quot;N-A&quot;</td><td>0</td><td>&quot;oil and natural gas corporatio…</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;ISO 3166&quot;</td><td>&quot;ISO 3166 is a standard made by…</td><td>490</td><td>81</td><td>&quot;N-A&quot;</td><td>0</td><td>&quot;iso 3166 is a standard made by…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌────────────┬────────────┬────────────┬────────────┬────────────┬───────┬────────────┬────────────┐\n",
       "│ filename   ┆ title      ┆ text       ┆ number_of_ ┆ number_of_ ┆ topic ┆ text_quali ┆ text_norma │\n",
       "│ ---        ┆ ---        ┆ ---        ┆ characters ┆ words      ┆ ---   ┆ ty         ┆ lized      │\n",
       "│ str        ┆ str        ┆ str        ┆ ---        ┆ ---        ┆ str   ┆ ---        ┆ ---        │\n",
       "│            ┆            ┆            ┆ i64        ┆ i64        ┆       ┆ i64        ┆ str        │\n",
       "╞════════════╪════════════╪════════════╪════════════╪════════════╪═══════╪════════════╪════════════╡\n",
       "│ demo_data_ ┆ Sélestat   ┆ Sélestat   ┆ 1522       ┆ 244        ┆ N-A   ┆ 0          ┆ s lestat   │\n",
       "│ downloads/ ┆            ┆ is a       ┆            ┆            ┆       ┆            ┆ is a       │\n",
       "│ wikipedia/ ┆            ┆ commune in ┆            ┆            ┆       ┆            ┆ commune in │\n",
       "│ …          ┆            ┆ north…     ┆            ┆            ┆       ┆            ┆ north…     │\n",
       "│ demo_data_ ┆ Kingdom of ┆ The        ┆ 3628       ┆ 602        ┆ N-A   ┆ 0          ┆ the        │\n",
       "│ downloads/ ┆ Asturias   ┆ Kingdom of ┆            ┆            ┆       ┆            ┆ kingdom of │\n",
       "│ wikipedia/ ┆            ┆ Asturias   ┆            ┆            ┆       ┆            ┆ asturias   │\n",
       "│ …          ┆            ┆ (; ) w…    ┆            ┆            ┆       ┆            ┆ was a …    │\n",
       "│ demo_data_ ┆ Oranienbur ┆ Oranienbur ┆ 698        ┆ 115        ┆ N-A   ┆ 0          ┆ oranienbur │\n",
       "│ downloads/ ┆ g          ┆ g (Slavic  ┆            ┆            ┆       ┆            ┆ g slavic   │\n",
       "│ wikipedia/ ┆            ┆ languages: ┆            ┆            ┆       ┆            ┆ languages  │\n",
       "│ …          ┆            ┆ …          ┆            ┆            ┆       ┆            ┆ b…         │\n",
       "│ demo_data_ ┆ Oil and    ┆ Oil and    ┆ 1148       ┆ 197        ┆ N-A   ┆ 0          ┆ oil and    │\n",
       "│ downloads/ ┆ Natural    ┆ Natural    ┆            ┆            ┆       ┆            ┆ natural    │\n",
       "│ wikipedia/ ┆ Gas Corpor ┆ Gas Corpor ┆            ┆            ┆       ┆            ┆ gas corpor │\n",
       "│ …          ┆ atio…      ┆ atio…      ┆            ┆            ┆       ┆            ┆ atio…      │\n",
       "│ demo_data_ ┆ ISO 3166   ┆ ISO 3166   ┆ 490        ┆ 81         ┆ N-A   ┆ 0          ┆ iso 3166   │\n",
       "│ downloads/ ┆            ┆ is a       ┆            ┆            ┆       ┆            ┆ is a       │\n",
       "│ wikipedia/ ┆            ┆ standard   ┆            ┆            ┆       ┆            ┆ standard   │\n",
       "│ …          ┆            ┆ made by…   ┆            ┆            ┆       ┆            ┆ made by…   │\n",
       "└────────────┴────────────┴────────────┴────────────┴────────────┴───────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Apply to entire dataframe\n",
    "df_normalized = df_original.with_columns(\n",
    "    pl.col(\"text\").map_elements(normalize_text, return_dtype=pl.Utf8).alias(\"text_normalized\")\n",
    ")\n",
    "\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Readability Filtering <a name=\"readability\"></a>\n",
    "\n",
    "The library uses the Flesch-Kincaid grade level to filter text by reading difficulty. This is useful for creating training data appropriate for specific education levels.\n",
    "# Flesch-Kincaid Grade Level\n",
    "\n",
    "The **Flesch-Kincaid Grade Level** is a readability test that indicates the U.S. school grade level required to understand a piece of text. It was developed by **Rudolf Flesch** and **J. Peter Kincaid** for the U.S. Navy in **1975**.\n",
    "\n",
    "## Formula\n",
    "\n",
    "The score is based on two key factors: sentence length and word complexity.\n",
    "\n",
    "**Grade Level = a × (total words / total sentences) + b × (total syllables / total words) - c**\n",
    "\n",
    "### Components\n",
    "\n",
    "- **Average Sentence Length** = total words / total sentences  \n",
    "  Longer sentences increase complexity.\n",
    "\n",
    "- **Average Syllables per Word** = total syllables / total words  \n",
    "  Words with more syllables indicate higher difficulty.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "| Score | Reading Level | Example Audience            |\n",
    "|-------|----------------|------------------------------|\n",
    "| 0–5   | Elementary     | 5th grade and below          |\n",
    "| 6–8   | Middle School  | 6th–8th grade                |\n",
    "| 9–12  | High School    | 9th–12th grade               |\n",
    "| 13–16 | College        | College undergraduate        |\n",
    "| 17+   | Graduate       | Graduate school and above    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original rows: 10000\n",
      "Filtered rows: 3125\n",
      "Removed: 6875 rows\n",
      "\n",
      "Remaining texts:\n",
      "shape: (3_125, 7)\n",
      "┌───────────────┬──────────────┬──────────────┬──────────────┬──────────────┬───────┬──────────────┐\n",
      "│ filename      ┆ title        ┆ text         ┆ number_of_ch ┆ number_of_wo ┆ topic ┆ text_quality │\n",
      "│ ---           ┆ ---          ┆ ---          ┆ aracters     ┆ rds          ┆ ---   ┆ ---          │\n",
      "│ str           ┆ str          ┆ str          ┆ ---          ┆ ---          ┆ str   ┆ i64          │\n",
      "│               ┆              ┆              ┆ i64          ┆ i64          ┆       ┆              │\n",
      "╞═══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═══════╪══════════════╡\n",
      "│ demo_data_dow ┆ Oranienburg  ┆ Oranienburg  ┆ 698          ┆ 115          ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆              ┆ (Slavic      ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ languages:…  ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Civil Works  ┆ The Civil    ┆ 745          ┆ 126          ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Administrati ┆ Works Admini ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆ on           ┆ stration…    ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Bokermannohy ┆ The Fazenda  ┆ 135          ┆ 23           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ la           ┆ Salto tree   ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆ ibitiguara   ┆ frog (\"…     ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Charnoz-sur- ┆ Charnoz-sur- ┆ 121          ┆ 20           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Ain          ┆ Ain is a     ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ commune. …   ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Borgholm     ┆ Borgholm is  ┆ 226          ┆ 43           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆              ┆ a town on    ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ the Swed…    ┆              ┆              ┆       ┆              │\n",
      "│ …             ┆ …            ┆ …            ┆ …            ┆ …            ┆ …     ┆ …            │\n",
      "│ demo_data_dow ┆ Vere Bird    ┆ Sir Vere     ┆ 292          ┆ 54           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆              ┆ Cornwall     ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ Bird Sr.,    ┆              ┆              ┆       ┆              │\n",
      "│               ┆              ┆ KN…          ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Red Rock,    ┆ Red Rock is  ┆ 52           ┆ 11           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Oklahoma     ┆ a town in    ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ Oklahoma…    ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Ana          ┆ Ana          ┆ 128          ┆ 22           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Destéfano    ┆ Destéfano is ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ an retired   ┆              ┆              ┆       ┆              │\n",
      "│               ┆              ┆ gy…          ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Lauro        ┆ Lauro Fred   ┆ 347          ┆ 62           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Cavazos      ┆ Cavazos Jr.  ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ (Januar…     ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Beauvois-en- ┆ Beauvois-en- ┆ 126          ┆ 20           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Vermandois   ┆ Vermandois   ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ is a co…     ┆              ┆              ┆       ┆              │\n",
      "└───────────────┴──────────────┴──────────────┴──────────────┴──────────────┴───────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "df_filtered = filter_by_readability(df_original, max_grade=8.0)\n",
    "\n",
    "print(f\"\\nOriginal rows: {len(df_original)}\")\n",
    "print(f\"Filtered rows: {len(df_filtered)}\")\n",
    "print(f\"Removed: {len(df_original) - len(df_filtered)} rows\")\n",
    "print(\"\\nRemaining texts:\")\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building CEFR-Based Synonym Dictionaries <a name=\"cefr-synonyms\"></a>\n",
    "\n",
    "For more vocabulary simplification, you can build synonym dictionaries based on CEFR (Common European Framework of Reference for Languages) levels. This replaces difficult words (B2, C1, C2) with easier synonyms (A1, A2).\n",
    "\n",
    "The library includes a script that uses WordNet and CEFR word lists to automatically generate these mappings.\n",
    "\n",
    "Dataset from https://www.kaggle.com/datasets/nezahatkk/10-000-english-words-cerf-labelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CEFR synonym dictionary built successfully!\n",
      "\n",
      "Output files:\n",
      "  - cefr_synonyms/synonyms.json  (for use in pipeline)\n",
      "  - cefr_synonyms/synonyms.csv   (detailed mapping)\n",
      "  - cefr_synonyms/unmapped.txt   (words without mappings)\n",
      "  - cefr_synonyms/build_stats.txt (statistics)\n"
     ]
    }
   ],
   "source": [
    "# Build a CEFR-based synonym dictionary\n",
    "#\n",
    "# This requires a CEFR wordlist CSV file. The library looks for it in common locations\n",
    "# or you can provide your own.\n",
    "#\n",
    "# The script will:\n",
    "# 1. Identify difficult words (B2, C1, C2 levels)\n",
    "# 2. Find easier synonyms from A1, A2 levels using WordNet\n",
    "# 3. Generate a JSON mapping file\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Note: You'll need a CEFR wordlist CSV file\n",
    "# The script can use common sources like Oxford 5000 or Cambridge wordlists\n",
    "# See CEFR_SYNONYMS.md for more information\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/build_synmap_from_cefr.py\",\n",
    "    \"--cefr_csv\", \"data/ENGLISH_CERF_WORDS.csv\",  # Provide your CEFR wordlist\n",
    "    \"--out_dir\", \"cefr_synonyms\",\n",
    "    \"--easy_levels\", \"A1,A2\",\n",
    "    \"--difficult_levels\", \"B2,C1,C2\"\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ CEFR synonym dictionary built successfully!\")\n",
    "    print(\"\\nOutput files:\")\n",
    "    print(\"  - cefr_synonyms/synonyms.json  (for use in pipeline)\")\n",
    "    print(\"  - cefr_synonyms/synonyms.csv   (detailed mapping)\")\n",
    "    print(\"  - cefr_synonyms/unmapped.txt   (words without mappings)\")\n",
    "    print(\"  - cefr_synonyms/build_stats.txt (statistics)\")\n",
    "else:\n",
    "    print(f\"✗ Build failed: {result.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CEFR Synonym Dictionary\n",
    "\n",
    "Once you've built a CEFR synonym dictionary, you can use it in your processing pipeline for intelligent vocabulary simplification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEFR-based vocabulary simplification:\n",
      "======================================================================\n",
      "\\nOriginal:   The physician utilized sophisticated equipment.\n",
      "Simplified: The physician utilized advanced equipment.\n",
      "\\nOriginal:   They commenced the endeavor immediately.\n",
      "Simplified: They commenced the endeavor immediately.\n",
      "\\nOriginal:   The automobile accelerated rapidly.\n",
      "Simplified: The automobile accelerated rapidly.\n",
      "\\n======================================================================\n",
      "Using CEFR synonyms in a pipeline:\n",
      "Starting pipeline with 1000 rows...\n",
      "Normalizing text...\n",
      "After normalization: 711 rows\n",
      "Applying synonym mapping...\n",
      "Deduplicating...\n",
      "Removed 0 duplicate rows, 711 remaining\n",
      "Pipeline complete! Final: 711 rows\n",
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ text                            │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ islamabad high court , located… │\n",
      "│ the houston dynamo are an amer… │\n",
      "│ campo ma an national park is a… │\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Example: Using a CEFR synonym dictionary\n",
    "# (This assumes you've built one using the script above)\n",
    "\n",
    "cefr_synonyms_file = \"cefr_synonyms/synonyms.json\"\n",
    "\n",
    "\n",
    "# Load and use the CEFR synonyms\n",
    "cefr_mapper = SynonymMapper.from_json(cefr_synonyms_file)\n",
    "\n",
    "# Test on some complex texts\n",
    "complex_texts = [\n",
    "    \"The physician utilized sophisticated equipment.\",\n",
    "    \"They commenced the endeavor immediately.\",\n",
    "    \"The automobile accelerated rapidly.\"\n",
    "]\n",
    "\n",
    "print(\"CEFR-based vocabulary simplification:\")\n",
    "print(\"=\"*70)\n",
    "for text in complex_texts:\n",
    "    simplified = cefr_mapper.simplify_line(text)\n",
    "    print(f\"\\\\nOriginal:   {text}\")\n",
    "    print(f\"Simplified: {simplified}\")\n",
    "\n",
    "# Use in a pipeline\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"Using CEFR synonyms in a pipeline:\")\n",
    "\n",
    "pipeline = DataPipeline().add_synonym_mapper(mapping_path=cefr_synonyms_file)\n",
    "df_cefr_simplified = pipeline.process(df_original)\n",
    "print(df_cefr_simplified.select([\"text\"]).head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Annotations <a name=\"custom\"></a>\n",
    "\n",
    "Add custom metadata to your text using annotation functions. This can include word counts, sentiment, complexity scores, or any other features you want to track.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text:\n",
      "  The cat sat on the mat. It was a sunny day.\n",
      "\n",
      "Extracted features:\n",
      "  word_count: 11\n",
      "  char_count: 43\n",
      "  sentence_count: 2\n",
      "  avg_word_length: 3.0\n",
      "  has_numbers: False\n",
      "\n",
      "======================================================================\n",
      "Applying custom annotations to all texts:\n",
      "======================================================================\n",
      "Starting pipeline with 1000 rows...\n",
      "Applying annotator 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 44106.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline complete! Final: 1000 rows\n",
      "\n",
      "Annotated dataframe (first 5 rows):\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ filename  ┆ title     ┆ text      ┆ number_of ┆ … ┆ char_coun ┆ sentence_ ┆ avg_word_ ┆ has_numb │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ _characte ┆   ┆ t         ┆ count     ┆ length    ┆ ers      │\n",
      "│ str       ┆ str       ┆ str       ┆ rs        ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│           ┆           ┆           ┆ ---       ┆   ┆ i64       ┆ i64       ┆ f64       ┆ bool     │\n",
      "│           ┆           ┆           ┆ i64       ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ demo_data ┆ Victoria  ┆ Victoria  ┆ 601       ┆ … ┆ 601       ┆ 5         ┆ 4.8       ┆ true     │\n",
      "│ _download ┆ Island    ┆ Island is ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆ an island ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆ i…        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ demo_data ┆ Accident  ┆ An        ┆ 2311      ┆ … ┆ 2311      ┆ 28        ┆ 5.1       ┆ false    │\n",
      "│ _download ┆           ┆ accident  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆ is when   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆ something ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆           ┆ …         ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ demo_data ┆ Farwell,  ┆ Farwell   ┆ 85        ┆ … ┆ 85        ┆ 4         ┆ 3.8       ┆ false    │\n",
      "│ _download ┆ Texas     ┆ is a city ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆ in the    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆ U.S. …    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ demo_data ┆ Wham!     ┆ Wham! was ┆ 491       ┆ … ┆ 491       ┆ 7         ┆ 4.3       ┆ true     │\n",
      "│ _download ┆           ┆ an        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆           ┆ English   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆           ┆ new       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆           ┆ roman…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ demo_data ┆ Puffin    ┆ Puffin    ┆ 208       ┆ … ┆ 208       ┆ 3         ┆ 4.2       ┆ true     │\n",
      "│ _download ┆ Rock and  ┆ Rock and  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s/wikiped ┆ the New   ┆ the New   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ ia/…      ┆ Friend…   ┆ Friend…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define custom annotation function\n",
    "def text_features(text: str) -> dict:\n",
    "    \"\"\"Extract custom features from text.\"\"\"\n",
    "    words = text.split()\n",
    "    sentences = text.count(\".\") + text.count(\"!\") + text.count(\"?\")\n",
    "    \n",
    "    return {\n",
    "        \"word_count\": len(words),\n",
    "        \"char_count\": len(text),\n",
    "        \"sentence_count\": max(1, sentences),  # At least 1\n",
    "        \"avg_word_length\": round(sum(len(w) for w in words) / len(words), 1) if words else 0,\n",
    "        \"has_numbers\": any(c.isdigit() for c in text),\n",
    "    }\n",
    "\n",
    "# Test on a sample text\n",
    "sample = \"The cat sat on the mat. It was a sunny day.\"\n",
    "features = text_features(sample)\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(f\"  {sample}\")\n",
    "print(\"\\nExtracted features:\")\n",
    "for key, value in features.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create annotator and apply to dataframe\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Applying custom annotations to all texts:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "annotator = CustomFunctionAnnotator(text_features)\n",
    "# Turn off normalization and dedup for this example to keep original text\n",
    "pipeline = DataPipeline(text_column=\"text\", normalize=False, dedup=False).add_annotator(annotator)\n",
    "df_annotated = pipeline.process(df_original)\n",
    "\n",
    "print(\"\\nAnnotated dataframe (first 5 rows):\")\n",
    "print(df_annotated.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Gemini for AI-Powered Annotations <a name=\"gemini\"></a>\n",
    "\n",
    "The library includes a built-in Google Gemini annotator that can automatically classify text by topic and education level using AI. This is more sophisticated than keyword-based classification.\n",
    "\n",
    "**Requirements**:\n",
    "- Install annotator dependencies: `uv pip install -e \".[annotators]\"`\n",
    "- Google API key: Set `GOOGLE_API_KEY` environment variable\n",
    "\n",
    "**What it does**:\n",
    "- Classifies text into 20+ topic categories (Mathematics, Computer Science, Life Sciences, etc.)\n",
    "- Determines education level (primary school, middle school, high school, university, PhD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini annotator initialized\n",
      "\\nProcessing texts with Gemini (this may take a minute)...\n",
      "Starting pipeline with 10000 rows...\n",
      "Applying annotator 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotating:   0%|▏                                                              | 20/10000 [00:12<1:41:18,  1.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20337/4136886985.py\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Process our sample data (this will call the Gemini API)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\nProcessing texts with Gemini (this may take a minute)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mdf_gemini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgemini_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\n✓ Gemini annotation complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mini_gpt_prep_lib/tiny_corpus_prep/pipeline.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Applying annotator {i+1}/{len(self.annotators)}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pipeline complete! Final: {len(df)} rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mini_gpt_prep_lib/tiny_corpus_prep/annotators.py\u001b[0m in \u001b[0;36mannotate_dataframe\u001b[0;34m(self, df, text_column, show_progress)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mini_gpt_prep_lib/tiny_corpus_prep/annotators.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             response = model.generate_content(\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 generation_config=self.genai.types.GenerationConfig(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_since_first_attempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompression\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     ) -> Any:\n\u001b[0;32m--> 277\u001b[0;31m         response, ignored_call = self._with_call(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m_with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_FailureOutcome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         call = self._interceptor.intercept_unary_unary(\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mcontinuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_call_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py\u001b[0m in \u001b[0;36mintercept_unary_unary\u001b[0;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontinuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_call_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogging_enabled\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: NO COVER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mresponse_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrailing_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36mcontinuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    313\u001b[0m             ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 response, call = self._thunk(new_method).with_call(\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mwith_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1193\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_ready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.10/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_call_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             )\n\u001b[0;32m-> 1162\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0m_handle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._interpret_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/tag.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._BatchOperationTag.event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/operation.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc.ReceiveInitialMetadataOperation.un_c\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi\u001b[0m in \u001b[0;36mgenexpr\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._metadatum\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_cls, key, value)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using Gemini Annotator\n",
    "#\n",
    "# This uses Google's Gemini AI to classify text by topic and education level\n",
    "# It's more accurate than keyword matching but requires an API key\n",
    "#\n",
    "# Setup:\n",
    "# 1. Get a Google API key from https://makersuite.google.com/app/apikey\n",
    "# 2. Set it as environment variable: export GOOGLE_API_KEY=\"your_key\"\n",
    "#    Or create a .env file with: GOOGLE_API_KEY=your_key\n",
    "# 3. Install dependencies: uv pip install -e \".[annotators]\"\n",
    "#\n",
    "\n",
    "from tiny_corpus_prep import GeminiAnnotator\n",
    "import os\n",
    "# \n",
    "# # Check if API key is available\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"MY_API_KEY\")\n",
    "# \n",
    "if api_key:\n",
    "    try:\n",
    "        # Initialize Gemini annotator\n",
    "        gemini = GeminiAnnotator(\n",
    "            model_name=\"gemini-2.5-flash-lite\",  # Fast, cost-effective model\n",
    "            temperature=0.1,  # Low temperature for consistent results\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Gemini annotator initialized\")\n",
    "        \n",
    "        # Create pipeline with Gemini\n",
    "        gemini_pipeline = (\n",
    "            DataPipeline(text_column=\"text\", normalize=False, dedup=False)\n",
    "            .add_annotator(gemini)\n",
    "        )\n",
    "        \n",
    "        # Process our sample data (this will call the Gemini API)\n",
    "        print(\"\\\\nProcessing texts with Gemini (this may take a minute)...\")\n",
    "        df_gemini = gemini_pipeline.process(df_original)\n",
    "        \n",
    "        print(\"\\\\n✓ Gemini annotation complete!\")\n",
    "        print(f\"\\\\nColumns: {df_gemini.columns}\")\n",
    "        \n",
    "        # Show results\n",
    "        print(\"\\\\nSample results:\")\n",
    "        print(df_gemini.select([\"text\", \"topic\", \"education\"]).head(5))\n",
    "        \n",
    "        # Show topic distribution\n",
    "        print(\"\\\\nTopic distribution:\")\n",
    "        topic_counts = df_gemini[\"topic\"].value_counts().sort(\"counts\", descending=True)\n",
    "        print(topic_counts)\n",
    "        \n",
    "        # Show education level distribution\n",
    "        print(\"\\\\nEducation level distribution:\")\n",
    "        edu_counts = df_gemini[\"education\"].value_counts().sort(\"counts\", descending=True)\n",
    "        print(edu_counts)\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"✗ Import error: {e}\")\n",
    "        print(\"\\\\nPlease install annotation dependencies:\")\n",
    "        print(\"  uv pip install -e '.[annotators]'\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "else:\n",
    "    print(\" GOOGLE_API_KEY not found in environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API Cost and Performance\n",
    "\n",
    "**Cost**: Gemini 2.5 Flash Lite is very affordable:\n",
    "- ~$0.15 per 1 million input tokens\n",
    "- Processing 10,000 texts (~50 words each) costs approximately $0.05\n",
    "\n",
    "**Performance**:\n",
    "- Processes ~10-20 texts per second\n",
    "- Can handle texts up to 15,000 characters\n",
    "\n",
    "**Tips**:\n",
    "- Use `gemini-2.5-flash-lite` for cost-effective processing\n",
    "- Set `temperature=0.1` for consistent results\n",
    "- Process in batches for large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline Example <a name=\"pipeline\"></a>\n",
    "\n",
    "Now let's combine multiple processing steps using the high-level `process_corpus` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus with:\n",
      "  - Text normalization\n",
      "  - Readability filter (max grade 10)\n",
      "  - Keyword filter (science, math, programming)\n",
      "  - Synonym mapping\n",
      "  - Deduplication\n",
      "\n",
      "======================================================================\n",
      "Reading input from: ../.cache/nanochat/base_data/shard_00000.parquet\n",
      "Loaded 53248 rows with columns: ['text']\n",
      "Starting pipeline with 53248 rows...\n",
      "Normalizing text...\n",
      "After normalization: 53248 rows\n",
      "Applying keyword filter (5 keywords)...\n",
      "After keyword filter: 8956 rows\n",
      "Applying readability filter (max grade: 10.0)...\n",
      "After readability filter: 2163 rows\n",
      "Applying synonym mapping...\n",
      "Deduplicating...\n",
      "Removed 0 duplicate rows, 2163 remaining\n",
      "Pipeline complete! Final: 2163 rows\n",
      "\n",
      "Writing output to: demo_processed.parquet\n",
      "Statistics written to: demo_processed.json\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Total rows in output: 2163\n",
      "Total columns: 1\n",
      "Columns: ['text']\n",
      "\n",
      "Processed data:\n",
      "shape: (2_163, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ text                            │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ better than the da vinci code   │\n",
      "│ …                               │\n",
      "│ the inuit tribe are indigenous… │\n",
      "│ from this article , students c… │\n",
      "│ foods from around the world     │\n",
      "│ a …                             │\n",
      "│ outdoor education is an effort… │\n",
      "│ …                               │\n",
      "│ victor leroy hill was born 19 … │\n",
      "│ have you ever poured sand out … │\n",
      "│ education in california scream… │\n",
      "│ introduction to the uk theory … │\n",
      "│ here are the factors that cont… │\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Process corpus with multiple filters\n",
    "print(\"Processing corpus with:\")\n",
    "print(\"  - Text normalization\")\n",
    "print(\"  - Readability filter (max grade 10)\")\n",
    "print(\"  - Keyword filter (science, math, programming)\")\n",
    "print(\"  - Synonym mapping\")\n",
    "print(\"  - Deduplication\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "stats = process_corpus(\n",
    "    input_path='../.cache/nanochat/base_data/shard_00000.parquet',\n",
    "    output_path=\"demo_processed.parquet\",\n",
    "    normalize=True,\n",
    "    max_grade=10.0,\n",
    "    keywords=[\"science\" \"math\", \"programming\", \"learning\", \"biology\", \"physics\"],\n",
    "    synonyms_map_path=\"./cefr_synonyms/synonyms.json\",\n",
    "    dedup=True,\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"\\nTotal rows in output: {stats['total_rows']}\")\n",
    "print(f\"Total columns: {stats['total_columns']}\")\n",
    "print(f\"Columns: {stats['columns']}\")\n",
    "\n",
    "# Load and display processed data\n",
    "df_processed = pl.read_parquet(\"demo_processed.parquet\")\n",
    "print(\"\\nProcessed data:\")\n",
    "print(df_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
