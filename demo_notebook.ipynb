{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook showcases the key features and processing steps of the **tiny_corpus_prep** library, a Polars-based corpus preparation tool for training tiny GPT-like models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation <a name=\"setup\"></a>\n",
    "\n",
    "First, let's import the necessary modules and verify the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import polars as pl\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import tiny_corpus_prep components (main API)\n",
    "from tiny_corpus_prep import (\n",
    "    process_corpus,\n",
    "    DataPipeline,\n",
    "    read_parquet,\n",
    "    write_parquet_with_stats,\n",
    "    filter_by_readability,\n",
    "    filter_by_keywords,\n",
    "    is_middle_school_level,\n",
    "    CustomFunctionAnnotator,\n",
    "    generate_stats,\n",
    ")\n",
    "\n",
    "# Import internal utilities (not part of public API but useful for demos)\n",
    "from tiny_corpus_prep.normalize import normalize_text\n",
    "from tiny_corpus_prep.synonyms import SynonymMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data <a name=\"download\"></a>\n",
    "\n",
    "Before processing, you can download real datasets from Wikipedia or FineWeb-edu. \n",
    "\n",
    "### Data Sources:\n",
    "- **Wikipedia**: Simple English Wikipedia - great for general knowledge\n",
    "- **FineWeb-edu**: Pre-filtered educational web content from HuggingFace\n",
    "\n",
    "**Note**: These downloads can be large (100MB - 2GB+) and may take several minutes. The cells below are commented out by default - uncomment to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data directory: demo_data_downloads\n",
      "Full path: /home/gillus/tiny_corpus_prep/demo_data_downloads\n"
     ]
    }
   ],
   "source": [
    "# Create a data directory for downloads\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"demo_data_downloads\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created data directory: {data_dir}\")\n",
    "print(f\"Full path: {data_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Download Wikipedia Data\n",
    "\n",
    "Downloads Simple English Wikipedia, extracts articles, and converts to parquet format.\n",
    "\n",
    "**Requirements**: `bzip2` (system), `wikiextractor` (Python package)\n",
    "\n",
    "**Size**: ~500MB compressed, expands to ~2GB\n",
    "\n",
    "Wikipedia content is released under the Creative Commons Attribution-ShareAlike (CC BY-SA 4.0) license. This means that any model trained on Wikipedia data, as well as any outputs that constitute derivative works, must comply with CC BY-SA requirements. In practice, you must ensure that attribution is preserved (e.g., acknowledging Wikipedia as a source in documentation), and any redistributed derivative content is shared under the same license. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "tiny_corpus_prep - Data Download Tool (Step 0)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Source: wikipedia\n",
      "  Output directory: demo_data_downloads/wikipedia\n",
      "  Date: 20251020\n",
      "\n",
      "\n",
      "=== Downloading Simple Wikipedia (20251020) ===\n",
      "✓ XML file already exists: demo_data_downloads/wikipedia/simplewiki-20251020-pages-articles-multistream.xml\n",
      "\n",
      "=== Extracting Wikipedia content ===\n",
      "Running WikiExtractor (output to demo_data_downloads/wikipedia/extracted)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Preprocessing 'demo_data_downloads/wikipedia/simplewiki-20251020-pages-articles-multistream.xml' to collect template definitions: this may take some time.\n",
      "INFO: Preprocessed 100000 pages\n",
      "INFO: Preprocessed 200000 pages\n",
      "INFO: Preprocessed 300000 pages\n",
      "INFO: Preprocessed 400000 pages\n",
      "INFO: Preprocessed 500000 pages\n",
      "INFO: Loaded 43176 templates in 23.2s\n",
      "INFO: Starting page extraction from demo_data_downloads/wikipedia/simplewiki-20251020-pages-articles-multistream.xml.\n",
      "INFO: Using 7 extract processes.\n",
      "INFO: Extracted 100000 articles (3791.7 art/s)\n",
      "INFO: Extracted 200000 articles (3879.7 art/s)\n",
      "INFO: Extracted 300000 articles (4301.3 art/s)\n",
      "INFO: Finished 7-process extraction of 382659 articles in 94.1s (4067.0 art/s)\n",
      "Processing files:   2%|▏         | 6/246 [00:00<00:04, 54.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Converting to Parquet ===\n",
      "Found 246 JSON files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 246/246 [00:04<00:00, 56.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created DataFrame with 382658 rows\n",
      "shape: (5, 7)\n",
      "┌───────────────┬──────────────┬──────────────┬──────────────┬──────────────┬───────┬──────────────┐\n",
      "│ filename      ┆ title        ┆ text         ┆ number_of_ch ┆ number_of_wo ┆ topic ┆ text_quality │\n",
      "│ ---           ┆ ---          ┆ ---          ┆ aracters     ┆ rds          ┆ ---   ┆ ---          │\n",
      "│ str           ┆ str          ┆ str          ┆ ---          ┆ ---          ┆ str   ┆ i64          │\n",
      "│               ┆              ┆              ┆ i64          ┆ i64          ┆       ┆              │\n",
      "╞═══════════════╪══════════════╪══════════════╪══════════════╪══════════���═══╪═══════╪══════════════╡\n",
      "│ demo_data_dow ┆ Beverly      ┆ Beverly      ┆ 262          ┆ 41           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Hills Madam  ┆ Hills Madam  ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ (also know…  ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ The Light at ┆ The Light at ┆ 269          ┆ 41           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ the Edge of  ┆ the Edge of  ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆ the W…       ┆ the W…       ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ INTLAB       ┆ INTLAB       ┆ 607          ┆ 90           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆              ┆ (INTerval    ┆              ┆              ���       ┆              │\n",
      "│ dia/…         ┆              ┆ LABoratory)  ┆              ┆              ┆       ┆              │\n",
      "│               ┆              ┆ i…           ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Lemon        ┆ Lemon        ┆ 263          ┆ 44           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Popsicle     ┆ Popsicle     ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆              ┆ (Israeli:    ┆              ┆              ┆       ┆              │\n",
      "│               ┆              ┆ Eskim…       ┆              ┆              ┆       ┆              │\n",
      "│ demo_data_dow ┆ Ace Eli and  ┆ Ace Eli and  ┆ 230          ┆ 37           ┆ N-A   ┆ 0            │\n",
      "│ nloads/wikipe ┆ Rodger of    ┆ Rodger of    ┆              ┆              ┆       ┆              │\n",
      "│ dia/…         ┆ the Skie…    ┆ the Skie…    ┆              ┆              ┆       ┆              │\n",
      "└───────────────┴──────────────┴──────────────┴──────────────┴──────────────┴───────┴──────────────┘\n",
      "\n",
      "✓ Saved to: demo_data_downloads/wikipedia/wikipedia.parquet\n",
      "\n",
      "============================================================\n",
      "✓ Download complete!\n",
      "============================================================\n",
      "\n",
      "Output file: demo_data_downloads/wikipedia/wikipedia.parquet\n",
      "\n",
      "Next step: Process with prepare_corpus.py\n",
      "Example:\n",
      "  python bin/prepare_corpus.py \\\n",
      "    --input demo_data_downloads/wikipedia/wikipedia.parquet \\\n",
      "    --output data/processed.parquet \\\n",
      "    --max-grade 8\n",
      "\n",
      "✓ Wikipedia download complete!\n",
      "File: demo_data_downloads/wikipedia/wikipedia.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download Wikipedia data\n",
    "# \n",
    "# This will:\n",
    "# 1. Download Simple Wikipedia dump (~500MB)\n",
    "# 2. Extract the XML file\n",
    "# 3. Use WikiExtractor to parse articles\n",
    "# 4. Convert to parquet format\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/download_data.py\",\n",
    "    \"--source\", \"wikipedia\",\n",
    "    \"--output-dir\", str(data_dir / \"wikipedia\"),\n",
    "    \"--date\", \"20251020\"  # You can change the date\n",
    "], capture_output=False)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ Wikipedia download complete!\")\n",
    "    wiki_file = data_dir / \"wikipedia\" / \"wikipedia.parquet\"\n",
    "    print(f\"File: {wiki_file}\")\n",
    "else:\n",
    "    print(f\"\\n Download failed with code {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Download FineWeb-edu Data\n",
    "\n",
    "Downloads pre-filtered educational web content from HuggingFace.\n",
    "\n",
    "**Requirements**: None (uses Python's `requests` library)\n",
    "\n",
    "**Size**: ~100MB per file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "tiny_corpus_prep - Data Download Tool (Step 0)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Source: fineweb\n",
      "  Output directory: demo_data_downloads/fineweb\n",
      "  Number of files: 1\n",
      "  Start index: 0\n",
      "\n",
      "\n",
      "=== Downloading FineWeb-edu ===\n",
      "Downloading 1 file(s) starting from index 0\n",
      "✓ File already exists: 000_00000.parquet\n",
      "\n",
      "✓ Downloaded to: demo_data_downloads/fineweb/000_00000.parquet\n",
      "\n",
      "❌ Error: Error creating dataset. Could not read schema from 'demo_data_downloads/fineweb/000_00000.parquet'. Is this a 'parquet' file?: Could not open Parquet input source 'demo_data_downloads/fineweb/000_00000.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n",
      "Error type: ArrowInvalid\n",
      "\n",
      "Full traceback:\n",
      "\n",
      "❌ Download failed with code 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gillus/tiny_corpus_prep/bin/download_data.py\", line 364, in main\n",
      "    output_file = download_fineweb(args.output_dir, args.num_files, args.start_index)\n",
      "  File \"/home/gillus/tiny_corpus_prep/bin/download_data.py\", line 285, in download_fineweb\n",
      "    preview_df = pl.read_parquet(result_path, use_pyarrow=True)\n",
      "  File \"/home/gillus/tiny_corpus_prep/.venv/lib/python3.13/site-packages/polars/_utils/deprecation.py\", line 128, in wrapper\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/home/gillus/tiny_corpus_prep/.venv/lib/python3.13/site-packages/polars/_utils/deprecation.py\", line 128, in wrapper\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/home/gillus/tiny_corpus_prep/.venv/lib/python3.13/site-packages/polars/io/parquet/functions.py\", line 241, in read_parquet\n",
      "    return _read_parquet_with_pyarrow(\n",
      "        source,\n",
      "    ...<4 lines>...\n",
      "        rechunk=rechunk,\n",
      "    )\n",
      "  File \"/home/gillus/tiny_corpus_prep/.venv/lib/python3.13/site-packages/polars/io/parquet/functions.py\", line 331, in _read_parquet_with_pyarrow\n",
      "    pa_table = pyarrow_parquet.read_table(\n",
      "        source_prep,\n",
      "    ...<2 lines>...\n",
      "        **pyarrow_options,\n",
      "    )\n",
      "  File \"/home/gillus/tiny_corpus_prep/.venv/lib/python3.13/site-packages/pyarrow/parquet/core.py\", line 1844, in read_table\n",
      "    dataset = ParquetDataset(\n",
      "        source,\n",
      "    ...<16 lines>...\n",
      "        arrow_extensions_enabled=arrow_extensions_enabled,\n",
      "    )\n",
      "  File \"/home/gillus/tiny_corpus_prep/.venv/lib/python3.13/site-packages/pyarrow/parquet/core.py\", line 1424, in __init__\n",
      "    self._dataset = ds.dataset(path_or_paths, filesystem=filesystem,\n",
      "                    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                               schema=schema, format=parquet_format,\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                               partitioning=partitioning,\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                               ignore_prefixes=ignore_prefixes)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gillus/tiny_corpus_prep/.venv/lib/python3.13/site-packages/pyarrow/dataset.py\", line 790, in dataset\n",
      "    return _filesystem_dataset(source, **kwargs)\n",
      "  File \"/home/gillus/tiny_corpus_prep/.venv/lib/python3.13/site-packages/pyarrow/dataset.py\", line 482, in _filesystem_dataset\n",
      "    return factory.finish(schema)\n",
      "           ~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"pyarrow/_dataset.pyx\", line 3226, in pyarrow._dataset.DatasetFactory.finish\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Error creating dataset. Could not read schema from 'demo_data_downloads/fineweb/000_00000.parquet'. Is this a 'parquet' file?: Could not open Parquet input source 'demo_data_downloads/fineweb/000_00000.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n"
     ]
    }
   ],
   "source": [
    "# Download FineWeb-edu data \n",
    "# This downloads educational web content from HuggingFace\n",
    "# Each file is ~100MB and contains pre-filtered educational text\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Download 2 files as an example (you can increase --num-files)\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/download_data.py\",\n",
    "    \"--source\", \"fineweb\",\n",
    "    \"--output-dir\", str(data_dir / \"fineweb\"),\n",
    "    \"--num-files\", \"1\",  # Download 2 files\n",
    "    \"--start-index\", \"0\"  # Start from file 0\n",
    "], capture_output=False)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ FineWeb download complete!\")\n",
    "    fineweb_file = data_dir / \"fineweb\" / \"fineweb_combined.parquet\"\n",
    "    print(f\"File: {fineweb_file}\")\n",
    "else:\n",
    "    print(f\"\\n Download failed with code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pl.read_parquet('demo_data_downloads/wikipedia/wikipedia.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10_000, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>filename</th><th>title</th><th>text</th><th>number_of_characters</th><th>number_of_words</th><th>topic</th><th>text_quality</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Emmy Award&quot;</td><td>&quot;The Emmy Awards are United Sta…</td><td>785</td><td>136</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Dooly (character)&quot;</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;My Ex and Whys&quot;</td><td>&quot;My Ex and Whys is a 2017 Filip…</td><td>238</td><td>39</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Victoria, Labuan&quot;</td><td>&quot;Victoria, or locally known as …</td><td>154</td><td>27</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Dobrogea&quot;</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Crocheting&quot;</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Les Mujouls&quot;</td><td>&quot;Les Mujouls is a commune. It i…</td><td>136</td><td>22</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Kids&#x27; Shows&quot;</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;Price elasticity of demand&quot;</td><td>&quot;In economics, the price elasti…</td><td>1528</td><td>263</td><td>&quot;N-A&quot;</td><td>0</td></tr><tr><td>&quot;demo_data_downloads/wikipedia/…</td><td>&quot;The Fall of Hyperion&quot;</td><td>&quot;The Fall of Hyperion is the se…</td><td>258</td><td>49</td><td>&quot;N-A&quot;</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10_000, 7)\n",
       "┌───────────────┬──────────────┬──────────────┬──────────────┬──────────────┬───────┬──────────────┐\n",
       "│ filename      ┆ title        ┆ text         ┆ number_of_ch ┆ number_of_wo ┆ topic ┆ text_quality │\n",
       "│ ---           ┆ ---          ┆ ---          ┆ aracters     ┆ rds          ┆ ---   ┆ ---          │\n",
       "│ str           ┆ str          ┆ str          ┆ ---          ┆ ---          ┆ str   ┆ i64          │\n",
       "│               ┆              ┆              ┆ i64          ┆ i64          ┆       ┆              │\n",
       "╞═══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═══════╪══════════════╡\n",
       "│ demo_data_dow ┆ Emmy Award   ┆ The Emmy     ┆ 785          ┆ 136          ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆ Awards are   ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ United Sta…  ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Dooly        ┆              ┆ 0            ┆ 0            ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ (character)  ┆              ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆              ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ My Ex and    ┆ My Ex and    ┆ 238          ┆ 39           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ Whys         ┆ Whys is a    ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ 2017 Filip…  ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Victoria,    ┆ Victoria, or ┆ 154          ┆ 27           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ Labuan       ┆ locally      ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ known as …   ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Dobrogea     ┆              ┆ 0            ┆ 0            ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆              ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆              ┆              ┆              ┆       ┆              │\n",
       "│ …             ┆ …            ┆ …            ┆ …            ┆ …            ┆ …     ┆ …            │\n",
       "│ demo_data_dow ┆ Crocheting   ┆              ┆ 0            ┆ 0            ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆              ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆              ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Les Mujouls  ┆ Les Mujouls  ┆ 136          ┆ 22           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆ is a         ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ commune. It  ┆              ┆              ┆       ┆              │\n",
       "│               ┆              ┆ i…           ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Kids' Shows  ┆              ┆ 0            ┆ 0            ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆              ┆              ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆              ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ Price        ┆ In           ┆ 1528         ┆ 263          ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ elasticity   ┆ economics,   ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆ of demand    ┆ the price    ┆              ┆              ┆       ┆              │\n",
       "│               ┆              ┆ elasti…      ┆              ┆              ┆       ┆              │\n",
       "│ demo_data_dow ┆ The Fall of  ┆ The Fall of  ┆ 258          ┆ 49           ┆ N-A   ┆ 0            │\n",
       "│ nloads/wikipe ┆ Hyperion     ┆ Hyperion is  ┆              ┆              ┆       ┆              │\n",
       "│ dia/…         ┆              ┆ the se…      ┆              ┆              ┆       ┆              │\n",
       "└───────────────┴──────────────┴──────────────┴──────────────┴──────────────┴───────┴──────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original = df_original.sample(10000)\n",
    "\n",
    "df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Normalization <a name=\"normalization\"></a>\n",
    "\n",
    "Text normalization includes lowercasing and cleaning up punctuation to create more consistent text for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply to entire dataframe\n",
    "df_normalized = df_original.with_columns(\n",
    "    pl.col(\"text\").map_elements(normalize_text, return_dtype=pl.Utf8).alias(\"text_normalized\")\n",
    ")\n",
    "\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Readability Filtering <a name=\"readability\"></a>\n",
    "\n",
    "The library uses the Flesch-Kincaid grade level to filter text by reading difficulty. This is useful for creating training data appropriate for specific education levels.\n",
    "# Flesch-Kincaid Grade Level\n",
    "\n",
    "The **Flesch-Kincaid Grade Level** is a readability test that indicates the U.S. school grade level required to understand a piece of text. It was developed by **Rudolf Flesch** and **J. Peter Kincaid** for the U.S. Navy in **1975**.\n",
    "\n",
    "## Formula\n",
    "\n",
    "The score is based on two key factors: sentence length and word complexity.\n",
    "\n",
    "**Grade Level = a × (total words / total sentences) + b × (total syllables / total words) - c**\n",
    "\n",
    "### Components\n",
    "\n",
    "- **Average Sentence Length** = total words / total sentences  \n",
    "  Longer sentences increase complexity.\n",
    "\n",
    "- **Average Syllables per Word** = total syllables / total words  \n",
    "  Words with more syllables indicate higher difficulty.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "| Score | Reading Level | Example Audience            |\n",
    "|-------|----------------|------------------------------|\n",
    "| 0–5   | Elementary     | 5th grade and below          |\n",
    "| 6–8   | Middle School  | 6th–8th grade                |\n",
    "| 9–12  | High School    | 9th–12th grade               |\n",
    "| 13–16 | College        | College undergraduate        |\n",
    "| 17+   | Graduate       | Graduate school and above    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = filter_by_readability(df_original, max_grade=8.0)\n",
    "\n",
    "print(f\"\\nOriginal rows: {len(df_original)}\")\n",
    "print(f\"Filtered rows: {len(df_filtered)}\")\n",
    "print(f\"Removed: {len(df_original) - len(df_filtered)} rows\")\n",
    "print(\"\\nRemaining texts:\")\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building CEFR-Based Synonym Dictionaries <a name=\"cefr-synonyms\"></a>\n",
    "\n",
    "For more vocabulary simplification, you can build synonym dictionaries based on CEFR (Common European Framework of Reference for Languages) levels. This replaces difficult words (B2, C1, C2) with easier synonyms (A1, A2).\n",
    "\n",
    "The library includes a script that uses WordNet and CEFR word lists to automatically generate these mappings.\n",
    "\n",
    "Dataset from https://www.kaggle.com/datasets/nezahatkk/10-000-english-words-cerf-labelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CEFR-based synonym dictionary\n",
    "#\n",
    "# This requires a CEFR wordlist CSV file. The library looks for it in common locations\n",
    "# or you can provide your own.\n",
    "#\n",
    "# The script will:\n",
    "# 1. Identify difficult words (B2, C1, C2 levels)\n",
    "# 2. Find easier synonyms from A1, A2 levels using WordNet\n",
    "# 3. Generate a JSON mapping file\n",
    "#\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Note: You'll need a CEFR wordlist CSV file\n",
    "# The script can use common sources like Oxford 5000 or Cambridge wordlists\n",
    "# See CEFR_SYNONYMS.md for more information\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"python\", \"bin/build_synmap_from_cefr.py\",\n",
    "    \"--cefr_csv\", \"data/ENGLISH_CERF_WORDS.csv\",  # Provide your CEFR wordlist\n",
    "    \"--out_dir\", \"cefr_synonyms\",\n",
    "    \"--easy_levels\", \"A1,A2\",\n",
    "    \"--difficult_levels\", \"B2,C1,C2\"\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ CEFR synonym dictionary built successfully!\")\n",
    "    print(\"\\nOutput files:\")\n",
    "    print(\"  - cefr_synonyms/synonyms.json  (for use in pipeline)\")\n",
    "    print(\"  - cefr_synonyms/synonyms.csv   (detailed mapping)\")\n",
    "    print(\"  - cefr_synonyms/unmapped.txt   (words without mappings)\")\n",
    "    print(\"  - cefr_synonyms/build_stats.txt (statistics)\")\n",
    "else:\n",
    "    print(f\"✗ Build failed: {result.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CEFR Synonym Dictionary\n",
    "\n",
    "Once you've built a CEFR synonym dictionary, you can use it in your processing pipeline for intelligent vocabulary simplification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using a CEFR synonym dictionary\n",
    "# (This assumes you've built one using the script above)\n",
    "\n",
    "cefr_synonyms_file = \"cefr_synonyms/synonyms.json\"\n",
    "\n",
    "\n",
    "# Load and use the CEFR synonyms\n",
    "cefr_mapper = SynonymMapper.from_json(cefr_synonyms_file)\n",
    "\n",
    "# Test on some complex texts\n",
    "complex_texts = [\n",
    "    \"The physician utilized sophisticated equipment.\",\n",
    "    \"They commenced the endeavor immediately.\",\n",
    "    \"The automobile accelerated rapidly.\"\n",
    "]\n",
    "\n",
    "print(\"CEFR-based vocabulary simplification:\")\n",
    "print(\"=\"*70)\n",
    "for text in complex_texts:\n",
    "    simplified = cefr_mapper.simplify_line(text)\n",
    "    print(f\"\\\\nOriginal:   {text}\")\n",
    "    print(f\"Simplified: {simplified}\")\n",
    "\n",
    "# Use in a pipeline\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"Using CEFR synonyms in a pipeline:\")\n",
    "\n",
    "pipeline = DataPipeline().add_synonym_mapper(mapping_path=cefr_synonyms_file)\n",
    "df_cefr_simplified = pipeline.process(df_original)\n",
    "print(df_cefr_simplified.select([\"text\"]).head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Annotations <a name=\"custom\"></a>\n",
    "\n",
    "Add custom metadata to your text using annotation functions. This can include word counts, sentiment, complexity scores, or any other features you want to track.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom annotation function\n",
    "def text_features(text: str) -> dict:\n",
    "    \"\"\"Extract custom features from text.\"\"\"\n",
    "    words = text.split()\n",
    "    sentences = text.count(\".\") + text.count(\"!\") + text.count(\"?\")\n",
    "    \n",
    "    return {\n",
    "        \"word_count\": len(words),\n",
    "        \"char_count\": len(text),\n",
    "        \"sentence_count\": max(1, sentences),  # At least 1\n",
    "        \"avg_word_length\": round(sum(len(w) for w in words) / len(words), 1) if words else 0,\n",
    "        \"has_numbers\": any(c.isdigit() for c in text),\n",
    "    }\n",
    "\n",
    "# Test on a sample text\n",
    "sample = \"The cat sat on the mat. It was a sunny day.\"\n",
    "features = text_features(sample)\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(f\"  {sample}\")\n",
    "print(\"\\nExtracted features:\")\n",
    "for key, value in features.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create annotator and apply to dataframe\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Applying custom annotations to all texts:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "annotator = CustomFunctionAnnotator(text_features)\n",
    "# Turn off normalization and dedup for this example to keep original text\n",
    "pipeline = DataPipeline(text_column=\"text\", normalize=False, dedup=False).add_annotator(annotator)\n",
    "df_annotated = pipeline.process(df_original)\n",
    "\n",
    "print(\"\\nAnnotated dataframe (first 5 rows):\")\n",
    "print(df_annotated.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Gemini for AI-Powered Annotations <a name=\"gemini\"></a>\n",
    "\n",
    "The library includes a built-in Google Gemini annotator that can automatically classify text by topic and education level using AI. This is more sophisticated than keyword-based classification.\n",
    "\n",
    "**Requirements**:\n",
    "- Install annotator dependencies: `uv pip install -e \".[annotators]\"`\n",
    "- Google API key: Set `GOOGLE_API_KEY` environment variable\n",
    "\n",
    "**What it does**:\n",
    "- Classifies text into 20+ topic categories (Mathematics, Computer Science, Life Sciences, etc.)\n",
    "- Determines education level (primary school, middle school, high school, university, PhD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gemini Annotator\n",
    "#\n",
    "# This uses Google's Gemini AI to classify text by topic and education level\n",
    "# It's more accurate than keyword matching but requires an API key\n",
    "#\n",
    "# Setup:\n",
    "# 1. Get a Google API key from https://makersuite.google.com/app/apikey\n",
    "# 2. Set it as environment variable: export GOOGLE_API_KEY=\"your_key\"\n",
    "#    Or create a .env file with: GOOGLE_API_KEY=your_key\n",
    "# 3. Install dependencies: uv pip install -e \".[annotators]\"\n",
    "#\n",
    "\n",
    "from tiny_corpus_prep import GeminiAnnotator\n",
    "import os\n",
    "# \n",
    "# # Check if API key is available\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"MY_API_KEY\")\n",
    "# \n",
    "if api_key:\n",
    "    try:\n",
    "        # Initialize Gemini annotator\n",
    "        gemini = GeminiAnnotator(\n",
    "            model_name=\"gemini-2.5-flash-lite\",  # Fast, cost-effective model\n",
    "            temperature=0.1,  # Low temperature for consistent results\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Gemini annotator initialized\")\n",
    "        \n",
    "        # Create pipeline with Gemini\n",
    "        gemini_pipeline = (\n",
    "            DataPipeline(text_column=\"text\", normalize=False, dedup=False)\n",
    "            .add_annotator(gemini)\n",
    "        )\n",
    "        \n",
    "        # Process our sample data (this will call the Gemini API)\n",
    "        print(\"\\\\nProcessing texts with Gemini (this may take a minute)...\")\n",
    "        df_gemini = gemini_pipeline.process(df_original)\n",
    "        \n",
    "        print(\"\\\\n✓ Gemini annotation complete!\")\n",
    "        print(f\"\\\\nColumns: {df_gemini.columns}\")\n",
    "        \n",
    "        # Show results\n",
    "        print(\"\\\\nSample results:\")\n",
    "        print(df_gemini.select([\"text\", \"topic\", \"education\"]).head(5))\n",
    "        \n",
    "        # Show topic distribution\n",
    "        print(\"\\\\nTopic distribution:\")\n",
    "        topic_counts = df_gemini[\"topic\"].value_counts().sort(\"counts\", descending=True)\n",
    "        print(topic_counts)\n",
    "        \n",
    "        # Show education level distribution\n",
    "        print(\"\\\\nEducation level distribution:\")\n",
    "        edu_counts = df_gemini[\"education\"].value_counts().sort(\"counts\", descending=True)\n",
    "        print(edu_counts)\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"✗ Import error: {e}\")\n",
    "        print(\"\\\\nPlease install annotation dependencies:\")\n",
    "        print(\"  uv pip install -e '.[annotators]'\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "else:\n",
    "    print(\" GOOGLE_API_KEY not found in environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API Cost and Performance\n",
    "\n",
    "**Cost**: Gemini 2.5 Flash Lite is very affordable:\n",
    "- ~$0.15 per 1 million input tokens\n",
    "- Processing 10,000 texts (~50 words each) costs approximately $0.05\n",
    "\n",
    "**Performance**:\n",
    "- Processes ~10-20 texts per second\n",
    "- Can handle texts up to 15,000 characters\n",
    "\n",
    "**Tips**:\n",
    "- Use `gemini-2.5-flash-lite` for cost-effective processing\n",
    "- Set `temperature=0.1` for consistent results\n",
    "- Process in batches for large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline Example <a name=\"pipeline\"></a>\n",
    "\n",
    "Now let's combine multiple processing steps using the high-level `process_corpus` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus with:\n",
      "  - Text normalization\n",
      "  - Readability filter (max grade 10)\n",
      "  - Keyword filter (science, math, programming)\n",
      "  - Synonym mapping\n",
      "  - Deduplication\n",
      "\n",
      "======================================================================\n",
      "Reading input from: ../.cache/nanochat/base_data/shard_00000.parquet\n",
      "Loaded 53248 rows with columns: ['text']\n",
      "Starting pipeline with 53248 rows...\n",
      "Normalizing text...\n",
      "After normalization: 53248 rows\n",
      "Applying keyword filter (5 keywords)...\n",
      "After keyword filter: 8956 rows\n",
      "Applying readability filter (max grade: 10.0)...\n",
      "After readability filter: 2163 rows\n",
      "Applying synonym mapping...\n",
      "Deduplicating...\n",
      "Removed 0 duplicate rows, 2163 remaining\n",
      "Pipeline complete! Final: 2163 rows\n",
      "\n",
      "Writing output to: demo_processed.parquet\n",
      "Statistics written to: demo_processed.json\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Total rows in output: 2163\n",
      "Total columns: 1\n",
      "Columns: ['text']\n",
      "\n",
      "Processed data:\n",
      "shape: (2_163, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ text                            │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ better than the da vinci code   │\n",
      "│ …                               │\n",
      "│ the inuit tribe are indigenous… │\n",
      "│ from this article , students c… │\n",
      "│ foods from around the world     │\n",
      "│ a …                             │\n",
      "│ outdoor education is an effort… │\n",
      "│ …                               │\n",
      "│ victor leroy hill was born 19 … │\n",
      "│ have you ever poured sand out … │\n",
      "│ education in california scream… │\n",
      "│ introduction to the uk theory … │\n",
      "│ here are the factors that cont… │\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Process corpus with multiple filters\n",
    "print(\"Processing corpus with:\")\n",
    "print(\"  - Text normalization\")\n",
    "print(\"  - Readability filter (max grade 10)\")\n",
    "print(\"  - Keyword filter (science, math, programming)\")\n",
    "print(\"  - Synonym mapping\")\n",
    "print(\"  - Deduplication\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "stats = process_corpus(\n",
    "    input_path='../.cache/nanochat/base_data/shard_00000.parquet',\n",
    "    output_path=\"demo_processed.parquet\",\n",
    "    normalize=True,\n",
    "    max_grade=10.0,\n",
    "    keywords=[\"science\" \"math\", \"programming\", \"learning\", \"biology\", \"physics\"],\n",
    "    synonyms_map_path=\"./cefr_synonyms/synonyms.json\",\n",
    "    dedup=True,\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"\\nTotal rows in output: {stats['total_rows']}\")\n",
    "print(f\"Total columns: {stats['total_columns']}\")\n",
    "print(f\"Columns: {stats['columns']}\")\n",
    "\n",
    "# Load and display processed data\n",
    "df_processed = pl.read_parquet(\"demo_processed.parquet\")\n",
    "print(\"\\nProcessed data:\")\n",
    "print(df_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
